{
 "metadata": {
  "name": "",
  "signature": "sha256:852849c59b7230fd6bae26e1c5d4db24f0242e96b4277e6f53fb01a99ceefad5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Combining Classifiers"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 10"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the second of two parts, where you are working together as a team to submit to a [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial) competition. You should already have produced some classifiers from the previous tutorial, and in this tutorial, you will combine them to further improve your submission. There are four different activities in this tutorial, and your tutor may wish to allocate different groups to different tasks.\n",
      "\n",
      "* Bagging and majority vote\n",
      "* Adaboost\n",
      "* More base classifiers\n",
      "* Generate better features\n",
      "\n",
      "As in the previous tutorial, use github to manage your code.\n",
      "\n",
      "## Data management\n",
      "\n",
      "Split the data into a training, validation and test sets with 10000, 10000 and 5000 examples respectively. Note that this constitutes [peeking](http://www.ajnrblog.org/2013/09/11/peeking-effect-supervised-feature-selection-diffusion-tensor-imaging-data/) since the features were constructed using the whole dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## (reminder) Kaggle data\n",
      "\n",
      "Each tutorial will be a team in the [Popcorn competition](https://www.kaggle.com/c/word2vec-nlp-tutorial). Your tutor will submit based on the code in the ```master``` branch. Please read the rules and description of the data.\n",
      "\n",
      "Your tutor will be in charge of the Kaggle submission. To help you get started quickly, you can download the bag of words data from the [course website](https://sml.forge.nicta.com.au/isml15/data/popcorn-features.csv.bz2). This is processed following the instructions [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words) from the Kaggle data, using the stop-words from sklearn. The file is compressed using bz2, and is a pandas dataframe.\n",
      "\n",
      "**Recall that the submission format should be a [csv file with two columns](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/evaluation). This should also be the format used for combining classifiers.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bagging and majority vote\n",
      "\n",
      "Recall from the lecture that bagging involves creating bootstrap samples from your training data and then combining their predictions by averaging. Implement three functions:\n",
      "\n",
      "1. A function that combines predictions using weighted majority vote. The input is a list of files containing predictions, and the output is a single file containing the aggregate prediction. An optional input is a list of weights that indicate how much you trust a particular set of predictions. You can assume that all the predictions are on the same examples, but you may wish to check that this is true and that the examples are in the same order. **This function will also be used by other groups.**\n",
      "2. A function that creates subsets of data from a given dataset. Assume that the input data is in a ```pandas``` CSV format, and write the output also in the same format. The subset of data is generated using the bootstrap approach, which is sampling with replacement. *Recall that in earlier tutorials, you had also written methods for splitting datasets. You may also wish to implement a function that returns a subset that performs sampling without replacement.*\n",
      "3. Use one of the classification algorithms from the previous tutorial (or from one of the other groups). By using the bootstrap approach on the training set, generate 10 classifiers which are trained on different subsets of the training data. Apply these 10 classifiers to the validation set and measure their AUC. Use the value of the AUC for each classifier on the validation set as the weight for the majority vote. Compare the performance of the 10 classifiers and the combined classifier on the test set. Also compute the majority vote with uniform weights. *Does the (weighted) majority vote improve performance?*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Adaboost\n",
      "\n",
      "Implement the Adaboost algorithm as presented in the lecture. **Use the majority vote function from the other group.**\n",
      "\n",
      "Compare the performance of Adaboost with a weighted majority vote based on AUC."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## More base classifiers\n",
      "\n",
      "*This option may require sudo access to install some dependencies.*\n",
      "\n",
      "[scikit-learn](http://scikit-learn.org/stable/install.html) has many [supervised learning methods](http://scikit-learn.org/stable/supervised_learning.html).\n",
      "\n",
      "Combining more diverse classifiers tend to result in better aggregate performance. Try using [random forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [nearest neighbour](http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html) which are very fast to train. Use the majority vote code from the other group to combine predictions. Does combining different classifiers improve performance?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exploring various feature construction methods\n",
      "\n",
      "This section is repeated from the previous tutorial.\n",
      "\n",
      "### Deep learning\n",
      "\n",
      "You may wish to try out various ways of generating features for your classifier. Follow the [Kaggle tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial), which includes descriptions of using [deep learning](https://code.google.com/p/word2vec/) to find good features.\n",
      "\n",
      "### n-gram models\n",
      "\n",
      "The bag of words representation ignores the order that words appear in a paragraph. One way to model this is to include pairs or triplets of words (called bi-grams and tri-grams). You may find the [following tutorial](http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/) useful for computing [n-grams](http://en.wikipedia.org/wiki/N-gram). The [Wikipedia article](http://en.wikipedia.org/wiki/N-gram) also gives some ideas for dealing with the zeros in your n-grams using smoothing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}