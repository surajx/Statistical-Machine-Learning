{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural Networks"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
      "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
      "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
      "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
      "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
      "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
      "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
      "\n",
      "Setting up the environment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The data set\n",
      "\n",
      "We will use an old dataset on the price of housing in Boston (see [description](https://archive.ics.uci.edu/ml/datasets/Housing)). The aim is to predict the median value of the owner occupied homes from various other factors. This is the same data as was used in Tutorial 2. However, this time we will explore data normalisation, and hence use the raw data instead. Please download this from [mldata.org](http://mldata.org/repository/data/download/csv/regression-datasets-housing/).\n",
      "\n",
      "As in Tutorial 2, use ```pandas``` to read the data. Remove the 'CHAS' feature from the dataset. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names = ['medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
      "data = pd.read_csv('regression-datasets-housing.csv', header=None, names=names)\n",
      "data.drop('chas', axis=1, inplace=True)\n",
      "data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "(506, 13)"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implement a function that will normalise each feature such that the mean value of the feature is zero and the variance is one. Apply this function to each feature in the housing dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Solution\n",
      "\n",
      "def normalise_z(data):\n",
      "    \"\"\"Returns data that is Z normalised.\n",
      "    Each feature has zero mean and unit variance.\n",
      "    \"\"\"\n",
      "    mu = np.mean(data, axis=0)\n",
      "    sigma = np.std(data, axis=0)\n",
      "    print(mu)\n",
      "    print(sigma)\n",
      "    assert np.any(sigma > 0.0), 'Zero variance'\n",
      "    return (data-mu)/sigma\n",
      "\n",
      "n_data = normalise_z(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "medv         3.613524\n",
        "crim        11.347826\n",
        "zn          11.136779\n",
        "indus        0.069170\n",
        "nox          6.284634\n",
        "rm          68.574901\n",
        "age          3.795043\n",
        "dis          9.549407\n",
        "rad        408.237154\n",
        "tax         18.083004\n",
        "ptratio    356.674032\n",
        "b           12.653063\n",
        "lstat       22.532806\n",
        "dtype: float64\n",
        "medv         8.593041\n",
        "crim        23.287547\n",
        "zn           6.853571\n",
        "indus        0.253743\n",
        "nox          0.701923\n",
        "rm          28.121033\n",
        "age          2.103628\n",
        "dis          8.698651\n",
        "rad        168.370495\n",
        "tax          2.278319\n",
        "ptratio     91.204607\n",
        "b            7.134002\n",
        "lstat        9.188012\n",
        "dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To simplify equations, we introduce an extra input so that the biases can be absorbed into the weights."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_ex = len(n_data.index)\n",
      "n_data['ones'] = np.ones(num_ex)\n",
      "n_data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>medv</th>\n",
        "      <th>crim</th>\n",
        "      <th>zn</th>\n",
        "      <th>indus</th>\n",
        "      <th>nox</th>\n",
        "      <th>rm</th>\n",
        "      <th>age</th>\n",
        "      <th>dis</th>\n",
        "      <th>rad</th>\n",
        "      <th>tax</th>\n",
        "      <th>ptratio</th>\n",
        "      <th>b</th>\n",
        "      <th>lstat</th>\n",
        "      <th>ones</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>-0.419782</td>\n",
        "      <td> 0.285654</td>\n",
        "      <td>-1.287909</td>\n",
        "      <td>-0.272599</td>\n",
        "      <td> 0.413672</td>\n",
        "      <td>-0.120013</td>\n",
        "      <td> 0.140214</td>\n",
        "      <td>-0.982843</td>\n",
        "      <td>-0.666608</td>\n",
        "      <td>-1.353192</td>\n",
        "      <td> 0.441052</td>\n",
        "      <td>-1.075562</td>\n",
        "      <td> 0.159686</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>-0.417339</td>\n",
        "      <td>-0.487292</td>\n",
        "      <td>-0.593381</td>\n",
        "      <td>-0.272599</td>\n",
        "      <td> 0.194274</td>\n",
        "      <td> 0.367166</td>\n",
        "      <td> 0.557160</td>\n",
        "      <td>-0.867883</td>\n",
        "      <td>-0.987329</td>\n",
        "      <td>-0.475352</td>\n",
        "      <td> 0.441052</td>\n",
        "      <td>-0.492439</td>\n",
        "      <td>-0.101524</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>-0.417342</td>\n",
        "      <td>-0.487292</td>\n",
        "      <td>-0.593381</td>\n",
        "      <td>-0.272599</td>\n",
        "      <td> 1.282714</td>\n",
        "      <td>-0.265812</td>\n",
        "      <td> 0.557160</td>\n",
        "      <td>-0.867883</td>\n",
        "      <td>-0.987329</td>\n",
        "      <td>-0.475352</td>\n",
        "      <td> 0.396427</td>\n",
        "      <td>-1.208727</td>\n",
        "      <td> 1.324247</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>-0.416750</td>\n",
        "      <td>-0.487292</td>\n",
        "      <td>-1.306878</td>\n",
        "      <td>-0.272599</td>\n",
        "      <td> 1.016303</td>\n",
        "      <td>-0.809889</td>\n",
        "      <td> 1.077737</td>\n",
        "      <td>-0.752922</td>\n",
        "      <td>-1.106115</td>\n",
        "      <td>-0.036432</td>\n",
        "      <td> 0.416163</td>\n",
        "      <td>-1.361517</td>\n",
        "      <td> 1.182758</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>-0.412482</td>\n",
        "      <td>-0.487292</td>\n",
        "      <td>-1.306878</td>\n",
        "      <td>-0.272599</td>\n",
        "      <td> 1.228577</td>\n",
        "      <td>-0.511180</td>\n",
        "      <td> 1.077737</td>\n",
        "      <td>-0.752922</td>\n",
        "      <td>-1.106115</td>\n",
        "      <td>-0.036432</td>\n",
        "      <td> 0.441052</td>\n",
        "      <td>-1.026501</td>\n",
        "      <td> 1.487503</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "       medv      crim        zn     indus       nox        rm       age  \\\n",
        "0 -0.419782  0.285654 -1.287909 -0.272599  0.413672 -0.120013  0.140214   \n",
        "1 -0.417339 -0.487292 -0.593381 -0.272599  0.194274  0.367166  0.557160   \n",
        "2 -0.417342 -0.487292 -0.593381 -0.272599  1.282714 -0.265812  0.557160   \n",
        "3 -0.416750 -0.487292 -1.306878 -0.272599  1.016303 -0.809889  1.077737   \n",
        "4 -0.412482 -0.487292 -1.306878 -0.272599  1.228577 -0.511180  1.077737   \n",
        "\n",
        "        dis       rad       tax   ptratio         b     lstat  ones  \n",
        "0 -0.982843 -0.666608 -1.353192  0.441052 -1.075562  0.159686     1  \n",
        "1 -0.867883 -0.987329 -0.475352  0.441052 -0.492439 -0.101524     1  \n",
        "2 -0.867883 -0.987329 -0.475352  0.396427 -1.208727  1.324247     1  \n",
        "3 -0.752922 -1.106115 -0.036432  0.416163 -1.361517  1.182758     1  \n",
        "4 -0.752922 -1.106115 -0.036432  0.441052 -1.026501  1.487503     1  "
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Comparing two normalisations\n",
      "\n",
      "Compare the normalised data ```n_data``` to the data from Tutorial 2 by plotting and/or comparing histograms. Discuss the potential effect of the normalisation on the regression task."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Solution\n",
      "\n",
      "data2 = pd.read_csv('housing_scale.csv', header=None, names=names)\n",
      "fig = plt.figure(figsize=(13,6))\n",
      "ax1 = fig.add_subplot(121)\n",
      "ax1.hist(n_data['crim'], bins=30)\n",
      "ax1.set_title('Z normalization')\n",
      "ax2 = fig.add_subplot(122)\n",
      "ax2.hist(data2['crim'], bins=30)\n",
      "ax2.set_title('interval [-1,1]')\n",
      "\n",
      "# Note that this is an open ended question."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<matplotlib.text.Text at 0x10a7e3550>"
       ]
      },
      {
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAwAAAAF6CAYAAACupz8AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10e7a1160>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Error Backpropagation\n",
      "\n",
      "Note that we are considering a regression problem. That is we want to predict the median value of homes (a real number) from the other features. We use the squared error to measure performance.\n",
      "$$\n",
      "E = \\frac{1}{2} \\sum_k (y_k - t_k)^2\n",
      "$$\n",
      "\n",
      "### Objective function\n",
      "Write down the objective function of a neural network with one hidden layer. Use the identity activation function for the hidden units. Write down the equation for 5 hidden units.\n",
      "\n",
      "How many input units should there be? What should be the activation function of the output units? Explain why these choices are reasonable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Solution\n",
      "\n",
      "Using notation from lecture slides:\n",
      "$$\n",
      "y_k(x,w) = g\\left(\\sum_{j=0}^M w_{kj}^{(2)} h \\left( \\sum_{i=0}^D w_{ji}^{(1)} x_i\\right)\\right).\n",
      "$$\n",
      "\n",
      "Since we are considering regression, $g(\\cdot)$ is the identity. \n",
      "We assume $h(\\cdot)$ is also the identity, simplifying matters.\n",
      "There should be 12 input units, one for each feature, and one output unit.\n",
      "$$\n",
      "y_1(x,w) = \\sum_{j=0}^4 w_{kj}^{(2)} \\left( \\sum_{i=0}^{11} w_{ji}^{(1)} x_i\\right).\n",
      "$$\n",
      "\n",
      "We consider the squared error, hence the objective function of the neural network is (where $t$ is the label):\n",
      "\\begin{align}\n",
      "E &= \\frac{1}{2} \\left(y_1(x,w) - t\\right)^2\\\\\n",
      "&= \\frac{1}{2} \\left(\\sum_{j=0}^4 w_{kj}^{(2)} \\left( \\sum_{i=0}^{11} w_{ji}^{(1)} x_i\\right) - t\\right)^2\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Gradient\n",
      "Compute the gradient\n",
      "$\\frac{\\partial E}{\\partial w^{(2)}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Solution\n",
      "$$\n",
      "\\frac{\\partial E}{\\partial w_{kj}^{(2)}} = \n",
      "\\left(\\sum_{j=0}^4 w_{kj}^{(2)} \\left( \\sum_{i=0}^{11} w_{ji}^{(1)} x_i\\right) - t\\right)\\left( \\sum_{i=0}^{11} w_{ji}^{(1)} x_i\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Checking correctness\n",
      "\n",
      "One strategy to check that your code is correct in neural networks (and in general any gradient code) is to numerically check that your expression is correct. From the lecture we see that:\n",
      "$$\n",
      "\\frac{\\partial E}{\\partial w^{(2)}} \\simeq \\frac{E(w^{(2)} + \\epsilon) - E(w^{(2)} - \\epsilon)}{2\\epsilon}.\n",
      "$$\n",
      "For more information see [the following wiki](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization).\n",
      "\n",
      "Implement two functions, one that computes the analytic gradient and the second that computes the numerical gradient."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Solution\n",
      "\n",
      "def grad_analytic(Wout, Whid, x_i, t):\n",
      "    \"\"\"Returns the gradient of the output layer, based on analytic formula.\"\"\"\n",
      "    hid = np.dot(Whid, x_i)\n",
      "    grad = (np.dot(Wout, hid) - t)*hid\n",
      "    return grad\n",
      "\n",
      "def objective(Wout, Whid, x_i, t):\n",
      "    \"\"\"Returns the objective value of the neural network\"\"\"\n",
      "    hid = np.dot(Whid, x_i)\n",
      "    obj = 0.5*(np.dot(Wout, hid) - t)**2\n",
      "    return obj\n",
      "\n",
      "def grad_numerical(Wout, Whid, x_i, t):\n",
      "    \"\"\"Returns the gradient of the output layer, based on numerical gradient\"\"\"\n",
      "    num_hidden = len(Wout)\n",
      "    grad = np.zeros(num_hidden)\n",
      "    for idx in range(num_hidden):\n",
      "        epsilon = 0.01\n",
      "        Wout_plus = Wout.copy()\n",
      "        Wout_plus[idx] += epsilon\n",
      "        Wout_minus = Wout.copy()\n",
      "        Wout_minus[idx] -= epsilon\n",
      "        grad[idx] = objective(Wout_plus, Whid, x_i, t) - objective(Wout_minus, Whid, x_i, t)\n",
      "        grad[idx] /= 2.*epsilon\n",
      "    return grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the Boston housing data above, confirm that the two functions return almost the same values of the gradient for various values of $w$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Solution\n",
      "    \n",
      "# Easiest test to debug is to use a deterministic function\n",
      "Wout = np.array([1,2,3,4,5], dtype=float)\n",
      "Whid = np.ones((5,13))\n",
      "sample_idx = 1\n",
      "x_i = np.array(n_data.iloc[sample_idx])[1:]\n",
      "print(objective(Wout, Whid, x_i, n_data['medv'][sample_idx]))\n",
      "print(grad_analytic(Wout, Whid, x_i, n_data['medv'][sample_idx]))\n",
      "print(grad_numerical(Wout, Whid, x_i, n_data['medv'][sample_idx]))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "321.434336157\n",
        "[ 43.56334992  43.56334992  43.56334992  43.56334992  43.56334992]\n",
        "[ 43.56334992  43.56334992  43.56334992  43.56334992  43.56334992]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## (optional) Gradients for hidden layer\n",
      "\n",
      "Derive and implement the gradients for the hidden layer, hence giving you the full two layer neural network. Use this with the experimental set up in Tutorial 2 to analyse the Boston housing data. Recall that since we are using linear activation functions, this is equivalent to using a linear model. Compare and contrast the results of the neural network with regularised linear regression."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}