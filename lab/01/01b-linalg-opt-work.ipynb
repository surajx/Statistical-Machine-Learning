{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra and Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up python environment ([do not use pylab](http://carreau.github.io/posts/10-No-PyLab-Thanks.ipynb.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.optimize as opt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following cost function $ f(X) $ defined\n",
    "over the space of real $ n \\times p $ matrices\n",
    "\\begin{equation}\n",
    "  f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F\n",
    "\\end{equation}\n",
    "where $ X \\in \\RR^{n \\times p} $, $ n \\ge p $, and the matrix $ C $ is symmetric, \n",
    "such that $ C = C^T $. The scalar $ \\mu $ is assumed to be larger than the $p$th smallest \n",
    "eigenvalue of $ C $. The matrix $ N $ is diagonal with distinct positive entries\n",
    "on the diagonal.\n",
    "The trace of a square matrix $ A $ is denoted as $ \\trace{A} $, and\n",
    "the Frobenius norm of an arbitrary matrix $ A $ is defined as $ \\Norm{A}_F = \\sqrt{\\trace{A^T A}} $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenious Norm\n",
    "\n",
    "Implement a Python function ```frobenius_norm``` which accepts an arbitrary matrix $ A $ and returns\n",
    "$ \\Norm{A}_F $ using the formula given. (Use ```numpy.trace``` and ```numpy.sqrt```.)\n",
    "1. Given a matrix $ A \\in \\RR^{n \\times p} $, what is the complexity of your implementation of ```frobenius_norm```\n",
    "using the formula above?\n",
    "2. Can you come up with a faster implementation, if you were additionally told that $ p \\ge n $ ?\n",
    "3. Can you find an even faster implementation than in 1. and 2.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "def frobenius_norm(A):\n",
    "    \"\"\" Accepts Matrix A and returns the Forbenius Norm of A\n",
    "        \n",
    "        (matrix) -> (number)\n",
    "    \"\"\"\n",
    "    return numpy.sqrt(numpy.trace(numpy.transpose(A) * A))\n",
    "\n",
    "def frobenius_norm_efficient(A):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    p = A.shape[1]\n",
    "    #return numpy.dot(numpy.vec(numpy.transpose(A)), numpy.vec(A))\n",
    "    return numpy.sqrt(sum([A[i,j]**2 for i in range(n) for j in range(p)]))\n",
    "\n",
    "print (frobenius_norm(numpy.mat([[1,2,3],[4,5,6],[7,8,9],[9,5,3]])))\n",
    "print (frobenius_norm_efficient(numpy.mat([[1,2,3],[4,5,6],[7,8,9],[9,5,3]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function $f(X)$ with matrix argument\n",
    "\n",
    "Implement the cost function defined as $f(X)$ above as a function ```cost_function_for_matrix```\n",
    "in Python.\n",
    "\n",
    "Hint: As good programmers, we do not use global variables in subroutines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost_function_for_matrix(X, mu):\n",
    "    C = construct_random_matrix()\n",
    "    N = construct_diagonal_matrix()\n",
    "    cost = 0.5 * numpy.trace(numpy.transpose(X) * C * X * N) + \\\n",
    "        mu * 0.25 * (frobenius_norm_efficient(N - numpy.transpose(X) * X) ** 2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function $f(X)$ with vector argument\n",
    "\n",
    "Many standard optimisation routines work only with vectors. Fortunately, as vector spaces,\n",
    "the space of matrices $ \\RR^{n \\times p} $ \n",
    "and the space of vectors $ \\RR^{n p} $ are isomorphic. What does this mean?\n",
    "\n",
    "Implement the cost function $ f(X) $ given $ X $ as a vector and call it ```cost_function_for_vector```.\n",
    "Which extra arguments do you need for the cost function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost_function_for_vector(x, mu):\n",
    "    x = numpy.mat(x).T\n",
    "    C = random_matrix_from_eigenvalues(numpy.random.randint(10, size=numpy.shape(x)[0]))\n",
    "    N = 10\n",
    "    cost = 0.5 * numpy.trace(x.T * C * x * N) + \\\n",
    "        mu * 0.25 * (N - numpy.dot(x.T,x))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of a random matrix $C$ with given eigenvalues\n",
    "\n",
    "A diagonal matrix has the nice property that the eigenvalues can be directly read off\n",
    "the diagonal. Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues, \n",
    "how many different diagonal matrices have the same set of eigenvalues?\n",
    "\n",
    "Ans: $n!$\n",
    "\n",
    "Given a diagonal matrix $ C \\in \\RR^{n \\times n} $ with distinct eigenvalues,\n",
    "how many different matrices have the same set of eigenvalues?\n",
    "\n",
    "Given a set of $ n $ distinct real eigenvalues $ \\mathcal{E} = \\{e_1, \\dots, e_n \\} $, \n",
    "write a Python function ```random_matrix_from_eigenvalues``` which takes a list of\n",
    "eigenvalues $ E $ and returns a random symmetric matrix $ C $ having the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43935 30956 29216 38050 37672 38081 28552 32337 31594 40431]\n",
      " [30956 26749 24128 28901 28474 29010 21368 23713 25134 31012]\n",
      " [29216 24128 25118 28218 28501 27375 23169 23304 19514 29699]\n",
      " [38050 28901 28218 42533 37690 36868 29692 36251 28310 39674]\n",
      " [37672 28474 28501 37690 37225 35095 29021 31868 26525 38537]\n",
      " [38081 29010 27375 36868 35095 35720 27145 31495 28949 37904]\n",
      " [28552 21368 23169 29692 29021 27145 24568 25062 17423 29703]\n",
      " [32337 23713 23304 36251 31868 31495 25062 31528 23802 33623]\n",
      " [31594 25134 19514 28310 26525 28949 17423 23802 31566 30605]\n",
      " [40431 31012 29699 39674 38537 37904 29703 33623 30605 41196]]\n"
     ]
    }
   ],
   "source": [
    "def random_matrix_from_eigenvalues(E: list) -> numpy.matrix:\n",
    "    rand_matrix = numpy.random.random_integers(1,20, (len(E), len(E)))\n",
    "    symm_matrix = rand_matrix + rand_matrix.T\n",
    "    diag_matrix = numpy.mat(numpy.diag(E))\n",
    "    return symm_matrix * diag_matrix * symm_matrix.T\n",
    "\n",
    "print (random_matrix_from_eigenvalues(numpy.random.randint(10, size=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function $f(X)$\n",
    "\n",
    "Use the minimisation functions ```fmin``` or ```fmin_powell``` provided in the\n",
    "Python package ```scipy.optimize``` to minimise the cost function ```cost_function_for_vector```.\n",
    "\n",
    "Hint: Use the argument ```args``` in the minimisation functions  ```fmin``` or ```fmin_powell``` \n",
    "to provide the extra parameters to\n",
    "your cost function ```cost_function_for_vector```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 159121464.620811\n",
      "         Iterations: 3\n",
      "         Function evaluations: 451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-50.64287727,  17.38196597,  -0.19701183,  80.94875723,\n",
       "           7.86485133,  25.36278399,  -9.92901549, -17.63793053,\n",
       "         -88.28335912, -19.36043994]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import fmin, fmin_powell\n",
    "x = numpy.mat(numpy.random.random_integers(1,20,10)).T\n",
    "#opt.fmin(cost_function_for_vector, x, (1,))\n",
    "fmin_powell(cost_function_for_vector, x, (1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of $f(X)$\n",
    "\n",
    "Calculate the gradient for the cost function $f(X)$ given the\n",
    "inner product on the space of real matrices $ n \\times p $ is defined as\n",
    "\\begin{equation}\n",
    "  \\inner{A}{B} = \\trace{A^T B}\n",
    "\\end{equation}\n",
    "\n",
    "Implement a function ```gradient_for_vector``` which takes $ X $ as a vector, and\n",
    "returns the gradient of $ f(X) $ as a vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description\n",
    "\n",
    "$$D(f(X), \\eta) = \\lim_{\\epsilon \\rightarrow 0} \\Big[\\frac{f(X + \\epsilon \\eta) - f(X)}{\\epsilon} \\Big]$$\n",
    "$f(X) = \\frac{1}{2} \\trace{X^T C X N} + \\mu \\frac{1}{4} \\Norm{N - X^T X}^2_F$\n",
    "\n",
    "$f(X + \\epsilon \\eta) = \\frac{1}{2} \\trace{(X + \\epsilon \\eta)^T C (X + \\epsilon \\eta) N} + \\mu \\frac{1}{4} \\Norm{N - (X + \\epsilon \\eta)^T (X + \\epsilon \\eta)}^2_F$\n",
    "\n",
    "$$\\lim_{\\epsilon \\rightarrow 0} \\Big[\\frac{f(X + \\epsilon \\eta) - f(X)}{\\epsilon} \\Big] = \\frac{1}{2} \\trace{X^T C \\eta N + \\eta^T CXN}$$\n",
    "\n",
    "$$\\Rightarrow D(f(X), \\eta) = \\frac{1}{2} \\big[\\trace{X^T C \\eta N} + \\trace{\\eta^T CXN}\\big]$$\n",
    "\n",
    "$= \\frac{1}{2} \\big[\\trace{X^T C \\eta N} + \\trace{N^T X^T C^T \\eta}\\big] \\tag{$\\trace{A^T} = \\trace{A}$}$\n",
    "\n",
    "$= \\frac{1}{2} \\big[\\trace{X^T C \\eta N} + \\trace{NX^T C \\eta}\\big]\\tag{$N$ is Diagonal, $C$ is symmetric}$\n",
    "\n",
    "$= \\frac{1}{2} \\big[\\trace{X^T C \\eta N} + \\trace{NX^T C \\eta}\\big]\\tag{$N$ is Diagonal, $C$ is symmetric}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the cost function $f(X)$ using the gradient\n",
    "\n",
    "Use the minimisation functions ```fmin_cg``` or ```fmin_bfgs``` provided in the\n",
    "Python package ```scipy.optimize``` to minimise the cost function ```cost_function_for_vector``` utilising the gradient ```gradient_for_vector```.\n",
    "\n",
    "Compare the speed of convergence to the minimisation with ```fmin``` or ```fmin_powell```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minima of $f(X)$\n",
    "\n",
    "Compare the columns $x_1,\\dots, x_p$ of the matrix $X^\\star$ which minimises $ f(X) $ \n",
    "\\begin{equation}\n",
    "  X^\\star = \\argmin_{X \\in \\RR^{n \\times p}} f(X)\n",
    "\\end{equation}\n",
    "\n",
    "with the eigenvectors related to the smallest eigenvalues of $ C $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
