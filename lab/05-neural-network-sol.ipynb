{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "We will use an old dataset on the price of housing in Boston (see [description](https://archive.ics.uci.edu/ml/datasets/Housing)). The aim is to predict the median value of the owner occupied homes from various other factors. This is the same data as was used in Tutorial 2. However, this time we will explore data normalisation, and hence use the raw data instead. Please download this from [mldata.org](http://mldata.org/repository/data/download/csv/regression-datasets-housing/).\n",
    "\n",
    "As in Tutorial 2, use ```pandas``` to read the data. Remove the 'CHAS' feature from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names =  ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat', 'medv']\n",
    "data = pd.read_csv('regression-datasets-housing.csv', header=None, names=names)\n",
    "data.drop('chas', axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that will normalise each feature such that the mean value of the feature is zero and the variance is one. Apply this function to each feature in the housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crim         3.613524\n",
      "zn          11.347826\n",
      "indus       11.136779\n",
      "nox          0.554695\n",
      "rm           6.284634\n",
      "age         68.574901\n",
      "dis          3.795043\n",
      "rad          9.549407\n",
      "tax        408.237154\n",
      "ptratio     18.083004\n",
      "b          356.674032\n",
      "lstat       12.653063\n",
      "medv        22.532806\n",
      "dtype: float64\n",
      "crim         8.593041\n",
      "zn          23.287547\n",
      "indus        6.853571\n",
      "nox          0.115763\n",
      "rm           0.701923\n",
      "age         28.121033\n",
      "dis          2.103628\n",
      "rad          8.698651\n",
      "tax        168.370495\n",
      "ptratio      2.278319\n",
      "b           91.204607\n",
      "lstat        7.134002\n",
      "medv         9.188012\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def normalise_z(data):\n",
    "    \"\"\"Returns data that is Z normalised.\n",
    "    Each feature has zero mean and unit variance.\n",
    "    \"\"\"\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    print(mu)\n",
    "    print(sigma)\n",
    "    assert np.any(sigma > 0.0), 'Zero variance'\n",
    "    return (data-mu)/sigma\n",
    "\n",
    "n_data = normalise_z(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify equations, we introduce an extra input so that the biases can be absorbed into the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "      <th>ones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419782</td>\n",
       "      <td>0.285654</td>\n",
       "      <td>-1.287909</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.353192</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.075562</td>\n",
       "      <td>0.159686</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.417339</td>\n",
       "      <td>-0.487292</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>0.194274</td>\n",
       "      <td>0.367166</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.475352</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.492439</td>\n",
       "      <td>-0.101524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.417342</td>\n",
       "      <td>-0.487292</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>1.282714</td>\n",
       "      <td>-0.265812</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.475352</td>\n",
       "      <td>0.396427</td>\n",
       "      <td>-1.208727</td>\n",
       "      <td>1.324247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416750</td>\n",
       "      <td>-0.487292</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.016303</td>\n",
       "      <td>-0.809889</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>-0.036432</td>\n",
       "      <td>0.416163</td>\n",
       "      <td>-1.361517</td>\n",
       "      <td>1.182758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412482</td>\n",
       "      <td>-0.487292</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.228577</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>-0.036432</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.026501</td>\n",
       "      <td>1.487503</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       crim        zn     indus       nox        rm       age       dis  \\\n",
       "0 -0.419782  0.285654 -1.287909 -0.144217  0.413672 -0.120013  0.140214   \n",
       "1 -0.417339 -0.487292 -0.593381 -0.740262  0.194274  0.367166  0.557160   \n",
       "2 -0.417342 -0.487292 -0.593381 -0.740262  1.282714 -0.265812  0.557160   \n",
       "3 -0.416750 -0.487292 -1.306878 -0.835284  1.016303 -0.809889  1.077737   \n",
       "4 -0.412482 -0.487292 -1.306878 -0.835284  1.228577 -0.511180  1.077737   \n",
       "\n",
       "        rad       tax   ptratio         b     lstat      medv  ones  \n",
       "0 -0.982843 -0.666608 -1.353192  0.441052 -1.075562  0.159686     1  \n",
       "1 -0.867883 -0.987329 -0.475352  0.441052 -0.492439 -0.101524     1  \n",
       "2 -0.867883 -0.987329 -0.475352  0.396427 -1.208727  1.324247     1  \n",
       "3 -0.752922 -1.106115 -0.036432  0.416163 -1.361517  1.182758     1  \n",
       "4 -0.752922 -1.106115 -0.036432  0.441052 -1.026501  1.487503     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ex = len(n_data.index)\n",
    "n_data['ones'] = np.ones(num_ex)\n",
    "n_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two normalisations\n",
    "\n",
    "Compare the normalised data ```n_data``` to the data from Tutorial 2 by plotting and/or comparing histograms. Discuss the potential effect of the normalisation on the regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1031425c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAv8AAAF6CAYAAACObjfUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSOsLkX5IkSeqI/x9RBGtdDWFCaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1083947f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "data2 = pd.read_csv('housing_scale.csv', header=None, names=names)\n",
    "fig = plt.figure(figsize=(13,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.hist(n_data['crim'], bins=30)\n",
    "ax1.set_title('Z normalization')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.hist(data2['crim'], bins=30)\n",
    "ax2.set_title('interval [-1,1]')\n",
    "\n",
    "# Note that this is an open ended question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Backpropagation\n",
    "\n",
    "Note that we are considering a regression problem. That is we want to predict the median value of homes (a real number) from the other features. We use the squared error to measure performance.\n",
    "$$\n",
    "E = \\frac{1}{2} \\sum_k (y_k - t_k)^2\n",
    "$$\n",
    "\n",
    "### Objective function\n",
    "Write down the objective function of a neural network with one hidden layer. Use the identity activation function for the hidden units. Write down the equation for 5 hidden units.\n",
    "\n",
    "How many input units should there be? What should be the activation function of the output units? Explain why these choices are reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Using notation from lecture slides:\n",
    "$$\n",
    "y_k(x,w) = g\\left(\\sum_{j=0}^M w_{kj}^{(2)} h \\left( \\sum_{i=0}^D w_{ji}^{(1)} x_i\\right)\\right).\n",
    "$$\n",
    "\n",
    "Since we are considering regression, $g(\\cdot)$ is the identity. \n",
    "We assume $h(\\cdot)$ is also the identity, simplifying matters.\n",
    "There should be 12 input units, one for each feature, and one output unit.\n",
    "$$\n",
    "y_1(x,w) = \\sum_{j=0}^4 w_{kj}^{(2)} \\left( \\sum_{i=0}^{11} w_{ji}^{(1)} x_i\\right).\n",
    "$$\n",
    "\n",
    "We consider the squared error, hence the objective function of the neural network is (where $t$ is the label):\n",
    "\\begin{align}\n",
    "E &= \\frac{1}{2} \\left(y_1(x,w) - t\\right)^2\\\\\n",
    "&= \\frac{1}{2} \\left(\\sum_{j=0}^4 w_{kj}^{(2)} \\left( \\sum_{i=0}^{11} w_{ji}^{(1)} x_i\\right) - t\\right)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "Compute the gradient\n",
    "$\\frac{\\partial E}{\\partial w^{(2)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{kj}^{(2)}} = \n",
    "\\left(\\sum_{j=0}^4 w_{kj}^{(2)} \\left( \\sum_{i=0}^{11} w_{ji}^{(1)} x_i\\right) - t\\right)\\left( \\sum_{i=0}^{11} w_{ji}^{(1)} x_i\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking correctness\n",
    "\n",
    "One strategy to check that your code is correct in neural networks (and in general any gradient code) is to numerically check that your expression is correct. From the lecture we see that:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^{(2)}} \\simeq \\frac{E(w^{(2)} + \\epsilon) - E(w^{(2)} - \\epsilon)}{2\\epsilon}.\n",
    "$$\n",
    "For more information see [the following wiki](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization).\n",
    "\n",
    "Implement two functions, one that computes the analytic gradient and the second that computes the numerical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def grad_analytic(Wout, Whid, x_i, t):\n",
    "    \"\"\"Returns the gradient of the output layer, based on analytic formula.\"\"\"\n",
    "    hid = np.dot(Whid, x_i)\n",
    "    grad = (np.dot(Wout, hid) - t)*hid\n",
    "    return grad\n",
    "\n",
    "def objective(Wout, Whid, x_i, t):\n",
    "    \"\"\"Returns the objective value of the neural network\"\"\"\n",
    "    hid = np.dot(Whid, x_i)\n",
    "    obj = 0.5*(np.dot(Wout, hid) - t)**2\n",
    "    return obj\n",
    "\n",
    "def grad_numerical(Wout, Whid, x_i, t):\n",
    "    \"\"\"Returns the gradient of the output layer, based on numerical gradient\"\"\"\n",
    "    num_hidden = len(Wout)\n",
    "    grad = np.zeros(num_hidden)\n",
    "    for idx in range(num_hidden):\n",
    "        epsilon = 0.01\n",
    "        Wout_plus = Wout.copy()\n",
    "        Wout_plus[idx] += epsilon\n",
    "        Wout_minus = Wout.copy()\n",
    "        Wout_minus[idx] -= epsilon\n",
    "        grad[idx] = objective(Wout_plus, Whid, x_i, t) - objective(Wout_minus, Whid, x_i, t)\n",
    "        grad[idx] /= 2.*epsilon\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Boston housing data above, confirm that the two functions return almost the same values of the gradient for various values of $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534.175027073\n",
      "[ 71.44456259  71.44456259  71.44456259  71.44456259  71.44456259]\n",
      "[ 71.44456259  71.44456259  71.44456259  71.44456259  71.44456259]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "    \n",
    "# Easiest test to debug is to use a deterministic function\n",
    "Wout = np.array([1,2,3,4,5], dtype=float)\n",
    "Whid = np.ones((5,13))\n",
    "sample_idx = 1\n",
    "x_i = np.array(n_data.iloc[sample_idx])[1:]\n",
    "print(objective(Wout, Whid, x_i, n_data['medv'][sample_idx]))\n",
    "print(grad_analytic(Wout, Whid, x_i, n_data['medv'][sample_idx]))\n",
    "print(grad_numerical(Wout, Whid, x_i, n_data['medv'][sample_idx]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Gradients for hidden layer\n",
    "\n",
    "Derive and implement the gradients for the hidden layer, hence giving you the full two layer neural network. Use this with the experimental set up in Tutorial 2 to analyse the Boston housing data. Recall that since we are using linear activation functions, this is equivalent to using a linear model. Compare and contrast the results of the neural network with regularised linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
