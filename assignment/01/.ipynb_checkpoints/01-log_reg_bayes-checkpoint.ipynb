{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Logistic Regression, Bayes Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Assignment 1 (due: Monday, 18 April, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Suraj Narayanan Sasikumar\n",
    "\n",
    "Student ID: u5881495"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "|             |Notes|\n",
    "|:------------|:--|\n",
    "|Maximum marks| 20|\n",
    "|Weight|20% of final grade|\n",
    "|Format| Complete this ipython notebook. Do not forget to fill in your name and student ID above|\n",
    "|Submission mode| Use [wattle](https://wattle.anu.edu.au/)|\n",
    "|Formulas| All formulas which you derive need to be explained unless you use very common mathematical facts. Picture yourself as explaining your arguments to somebody who is just learning about your assignment. With other words, do not assume that the person marking your assignment knows all the background and therefore you can just write down the formulas without any explanation. It is your task to convince the reader that you know what you are doing when you derive an argument. Typeset all formulas in $\\LaTeX$.|\n",
    "| Code quality | Python code should be well structured, use meaningful identifiers for variables and subroutines, and provide sufficient comments. Please refer to the examples given in the tutorials. |\n",
    "| Code efficiency | An efficient implementation of an algorithm uses fast subroutines provided by the language or additional libraries. For the purpose of implementing Machine Learning algorithms in this course, that means using the appropriate data structures provided by Python and in numpy/scipy (e.g. Linear Algebra and random generators). |\n",
    "| Late penalty | For every day (starts at midnight) after the deadline of an assignment, the mark will be reduced by 5%. No assignments shall be accepted if it is later than 10 days. | \n",
    "| Coorperation | All assignments must be done individually. Cheating and plagiarism will be dealt with in accordance with University procedures (please see the ANU policies on [Academic Honesty and Plagiarism](http://academichonesty.anu.edu.au)). Hence, for example, code for programming assignments must not be developed in groups, nor should code be shared. You are encouraged to broadly discuss ideas, approaches and techniques with a few other students, but not at a level of detail where specific solutions or implementation issues are described by anyone. If you choose to consult with other students, you will include the names of your discussion partners for each solution. If you have any questions on this, please ask the lecturer before you act. |\n",
    "| Solution | To be presented in the tutorials. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment has two parts. In the first part, you apply logistic regression to given data (maximal 13 marks). In the second part, you answer a number of questions (maximal 7 marks). All formulas and calculations which are not part of Python code should be written using $\\LaTeX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\dotprod}[2]{\\left\\langle #1, #2 \\right\\rangle}$\n",
    "$\\newcommand{\\onevec}{\\mathbb{1}}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "\n",
    "The data set contains mass-spectrometric data which are used to distinguish between cancer and normal patterns (https://archive.ics.uci.edu/ml/datasets/Arcene). \n",
    "\n",
    "Please download the following data:\n",
    "* training data https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.data,\n",
    "* training labels https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.labels,\n",
    "* validation data https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_valid.data, and\n",
    "* validation labels https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/arcene_valid.labels.\n",
    "\n",
    "The following code reads the training and validation data into your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10000)\n",
      "(100, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.loadtxt(\"arcene_train.data\")\n",
    "y_train = np.loadtxt(\"arcene_train.labels\")\n",
    "X_val   = np.loadtxt(\"arcene_valid.data\")\n",
    "y_val   = np.loadtxt(\"arcene_valid.labels\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the 10000-dimensional input space might lead to long computation times, we have prepared a subset of 220 features. Unless otherwise stated, you will from now on work only with these subset of the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 220)\n",
      "(100, 220)\n"
     ]
    }
   ],
   "source": [
    "feature_mask = np.array(\n",
    "    [   4,   37,   85,  187,  233,  255,  375,  413,  435,  468,\n",
    "      470,  477,  519,  528,  628,  629,  643,  661,  678,  682,\n",
    "      750,  783,  786,  802,  936,  997, 1035, 1043, 1113, 1186,\n",
    "     1288, 1294, 1316, 1319, 1441, 1475, 1488, 1546, 1577, 1589,\n",
    "     1666, 1671, 1739, 1761, 1830, 1848, 1881, 1882, 1975, 2057,\n",
    "     2116, 2157, 2170, 2297, 2308, 2406, 2407, 2460, 2532, 2619,\n",
    "     2632, 2634, 2644, 2656, 2697, 2717, 2771, 2817, 2865, 2937,\n",
    "     3006, 3033, 3109, 3250, 3256, 3364, 3369, 3386, 3517, 3574,\n",
    "     3611, 3643, 3660, 3702, 3777, 3783, 3856, 4008, 4016, 4036,\n",
    "     4058, 4081, 4147, 4157, 4182, 4197, 4202, 4230, 4251, 4305,\n",
    "     4379, 4440, 4454, 4467, 4485, 4555, 4557, 4579, 4585, 4607,\n",
    "     4685, 4702, 4721, 4730, 4894, 4899, 4954, 4959, 5004, 5048,\n",
    "     5076, 5200, 5230, 5242, 5249, 5306, 5355, 5472, 5476, 5631,\n",
    "     5720, 5773, 5790, 5936, 5994, 6106, 6111, 6162, 6163, 6192,\n",
    "     6304, 6350, 6402, 6407, 6439, 6462, 6480, 6494, 6522, 6555,\n",
    "     6596, 6620, 6678, 6773, 6791, 6869, 6888, 6889, 6904, 6927,\n",
    "     6957, 6961, 7101, 7196, 7214, 7271, 7279, 7297, 7425, 7431,\n",
    "     7436, 7462, 7505, 7512, 7627, 7651, 7747, 7793, 7812, 7855,\n",
    "     7856, 7860, 7866, 7932, 7976, 7993, 8006, 8131, 8155, 8257,\n",
    "     8266, 8270, 8367, 8378, 8440, 8472, 8501, 8726, 8761, 8829,\n",
    "     8831, 8903, 9021, 9024, 9026, 9060, 9081, 9116, 9211, 9214,\n",
    "     9233, 9319, 9371, 9506, 9539, 9549, 9603, 9616, 9633, 9703])\n",
    "X_train_sub = X_train[:, feature_mask]\n",
    "X_val_sub   = X_val[:, feature_mask]\n",
    "\n",
    "print(X_train_sub.shape)\n",
    "print(X_val_sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Normalise the input data\n",
    "Find a linear transformation of the training data resulting in a zero mean and unit variance. Report the parameters of the linear transformation for the first ten dimensions of the input data.\n",
    "\n",
    "In general: \n",
    "* Under which circumstances does working with this transformed data lead to an advantage? \n",
    "* When is it counterproductive to normalise input data to zero mean and/or to unit variance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean for first 10 dimension: [17.109999999999999, 2.4199999999999999, 19.399999999999999, 2.8199999999999998, 35.850000000000001, 24.829999999999998, 70.010000000000005, 84.939999999999998, 20.059999999999999, 16.98]\n",
      "Standard Deviation for first 10 dimension: [26.215985581320417, 8.2694377076074517, 24.963172875257666, 9.0811673258452856, 42.418716387934232, 28.888078856164874, 75.096803527180839, 86.423934184923567, 25.67637824927807, 26.601872114571183]\n",
      "[[-0.65265523 -0.29264384 -0.7771448  ..., -0.95926957  1.57272964\n",
      "   1.67092457]\n",
      " [ 1.6360247  -0.29264384 -0.7771448  ...,  0.20705545  1.61340018\n",
      "  -0.59328979]\n",
      " [-0.65265523 -0.29264384 -0.7771448  ..., -0.88064092  2.34546983\n",
      "  -0.59328979]\n",
      " ..., \n",
      " [-0.65265523 -0.29264384 -0.17625965 ..., -0.91995525 -0.50146771\n",
      "  -0.59328979]\n",
      " [ 1.97932669 -0.29264384 -0.21631866 ..., -0.67096451  1.12535374\n",
      "  -0.59328979]\n",
      " [-0.65265523 -0.29264384 -0.7771448  ...,  0.32499843  0.10859033\n",
      "  -0.59328979]]\n"
     ]
    }
   ],
   "source": [
    "X_train_sub_mean = [ X_train_sub[:,col].mean() for col in np.arange(X_train_sub.shape[1])]\n",
    "X_train_sub_sd = [ X_train_sub[:,col].std() for col in np.arange(X_train_sub.shape[1])]\n",
    "\n",
    "print(\"Mean for first 10 dimension:\", X_train_sub_mean[:10])\n",
    "print(\"Standard Deviation for first 10 dimension:\", X_train_sub_sd[:10])\n",
    "\n",
    "def normalize(X):\n",
    "    return (X - X_train_sub_mean)/X_train_sub_sd\n",
    "\n",
    "X_train_sub_norm = normalize(X_train_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Error function for logistic regression with quadratic regularisation\n",
    "Define a Python function calculating the *cross-entropy* $ E(\\mathbf{w}) = - \\ln p(\\mathbf{t} \\;|\\; \\mathbf{w}) $for logistic regression with two classes and quadratic (l2) regularisation for a given parameter vector $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Gradient for logistic regression with quadratic regularisation\n",
    "Define a Python function calculating the gradient of the above *cross-entropy* for logistic regression with two classes and quadratic (l2) regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Finding the optimal parameters for given regularisation\n",
    "Using the error function and the gradient defined above, you now setup the optimisation finding the optimal parameter vector $\\mathbf{w}^\\star$ for the training data and a fixed regularisation constant $\\lambda $.\n",
    "Use the function *scipy.optimize.fmin_bfgs* as optimiser.\n",
    "\n",
    "For each $\\lambda= 10^k, k=-3,-2,..,1$, report the first 10 components of the optimal parameter $\\mathbf{w}^\\star$ found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Evaluating the solution with the validation data\n",
    "So far, you have only used the training data. You now apply the learned model to the validation data and compare the prediction you get with the given validation labels. \n",
    "\n",
    "Report the performance measures\n",
    "* the number of false positives (FP),\n",
    "* the number of false negatives (FN),\n",
    "* the number of true positives (TP),\n",
    "* the number of true negatives (TN),\n",
    "* the error rate,\n",
    "* the specificity, \n",
    "* the sensitivity,\n",
    "\n",
    "for the different settings of $\\lambda = 10^k, k=-3,-2,..,1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Finding an optimal regularisation constant\n",
    "We now consider the regularisation constant as a hyperparameter which we want to optimise.\n",
    "For this task we use the training and validation data together.\n",
    "\n",
    "Implement *s-fold cross-validation* with $s = 10$ to find an optimal regularisation constant which further reduces the error rates found in the previous question. Report the \n",
    "* optimal setting for the regularisation constant,\n",
    "* the first 10 components of the optimal parameter $\\mathbf{w}^\\star$, and \n",
    "* the same performance measures as specified in the previous question which you achieved with those settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Feature selection from all 10000 features\n",
    "\n",
    "In this task, you will use all 10000 features of the input data.\n",
    "\n",
    "The goal is to find a subset of the 10000 features which improves the solution found so far.\n",
    "\n",
    "An improvement is when at least one of\n",
    "* the error, or\n",
    "* the size of your chosen subset\n",
    "\n",
    "decreases.\n",
    "\n",
    "Please explain your approach and the reasons why you have chosen it.\n",
    "Provide code to find the subset and report the number of features and the error you are able to achive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Maximum likelihood and maximum a posteriori (MAP)\n",
    "We assume data samples $X_n = \\{ x_1,\\dots,x_n \\}$ were generated i.i.d. from a uniform distribution with unknown positive parameter $\\theta$:\n",
    "$$\n",
    "   \\mathcal{U}(x \\;|\\; 0, \\theta) = \n",
    "\\begin{cases}\n",
    " 1/\\theta & 0 \\leq x \\leq \\theta \\\\\n",
    " 0        & \\textrm{otherwise}   \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "a) We now observe four data samples $ X_4 = \\{ 6, 8, 9, 5\\}$.\n",
    "Calculate $\\theta_{ML}$, the maximum likelihood estimate of $\\theta$ for the observed data.\n",
    "\n",
    "b) Calculate the posterior distribution of $\\theta$ \n",
    "given that the data $ X_4 = \\{ 6, 8, 9, 5 \\}$ have been observed. As prior for $\\theta$\n",
    "use $p(\\theta) = \\mathcal{U}(x \\;|\\; 0, 10)$.\n",
    "\n",
    "c) Calculate $\\theta_{MAP}$, the maximum a posteriori estimate of $\\theta$ given the data $ X_4 $ and the prior $p(\\theta)$ as in the previous question.\n",
    "\n",
    "Write down the calculations in $\\LaTeX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Variance of sum of random vartiables\n",
    "Prove that the following holds for the variance of a sum of two random variables\n",
    "$ X $ and $ Y $\n",
    "$$\n",
    "\\operatorname{var}[X + Y] = \\operatorname{var}[X] + \\operatorname{var}[Y] + 2 \\operatorname{cov}[X,Y],\n",
    "$$\n",
    "where $ \\operatorname{cov}[X,Y] $ is the covariance between $X$ and $Y$.\n",
    "  \n",
    "For each step in your proof, provide a verbal explanation why this transformation step holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Matrix-vector identity proof\n",
    "Given a nonsingular matrix $ \\mathbf{A} $ and a vector $ \\mathbf{v} $ of comparable\n",
    "dimension, prove the following identity:\n",
    "$$\n",
    " (\\mathbf{A} + \\mathbf{v} \\mathbf{v}^T)^{-1} \n",
    "   = \\mathbf{A}^{-1} - \\frac{(\\mathbf{A}^{-1} \\mathbf{v}) (\\mathbf{v}^T \\mathbf{A}^{-1})}\n",
    "                       {1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Change of variance\n",
    "In Bayesian Linear Regression, the predictive distribution \n",
    "with a simplified prior \n",
    "  $ p(\\mathbf{w}  \\;|\\;  \\alpha) = \\mathcal{N}(\\mathbf{w} \\;|\\; \\mathbf{0}, \\alpha^{-1}\\mathbf{I}) $\n",
    "is a Gaussian distribution,\n",
    "$$ \n",
    "p(t  \\;|\\;  \\mathbf{x}, \\mathbf{t}, \\alpha, \\beta) \n",
    "= \\mathcal{N} (t \\;|\\; \\mathbf{m}_N^T \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x}), \\sigma_N^2(\\mathbf{x})) \n",
    "$$\n",
    "with variance\n",
    "$$\n",
    "  \\sigma_N^2(\\mathbf{x}) = \\frac{1}{\\beta} + \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x})^T \\mathbf{S}_N \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "After using another training pair $ \\left( \\mathbf{x}_{N+1}, t_{N+1} \\right) $ to adapt ($=$learn) the model,\n",
    "the variance of the predictive distribution becomes\n",
    "\n",
    "$$\n",
    "  \\sigma_{N+1}^2(\\mathbf{x}) = \\frac{1}{\\beta} + \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x})^T \\mathbf{S}_{N+1} \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "a) Define the dimensions of the variables.\n",
    "\n",
    "b) Prove that the uncertainties $ \\sigma_N^2(\\mathbf{x}) $ and\n",
    "$ \\sigma_{N+1}^2(\\mathbf{x}) $ associated with the\n",
    "predictive distributions satisfy\n",
    "\n",
    "$$\n",
    "  \\sigma_{N+1}^2(\\mathbf{x}) \\le \\sigma_N^2(\\mathbf{x}).\n",
    "$$\n",
    "*Hint: Use the Matrix-vector identity proved in the previous question.*\n",
    "\n",
    "c) Explain the meaning of this inequality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
