{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification with Logistic Regression, Bayes Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Assignment 1 (due: Monday, 18 April, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Suraj Narayanan Sasikumar\n",
    "\n",
    "Student ID: u5881495"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "|             |Notes|\n",
    "|:------------|:--|\n",
    "|Maximum marks| 20|\n",
    "|Weight|20% of final grade|\n",
    "|Format| Complete this ipython notebook. Do not forget to fill in your name and student ID above|\n",
    "|Submission mode| Use [wattle](https://wattle.anu.edu.au/)|\n",
    "|Formulas| All formulas which you derive need to be explained unless you use very common mathematical facts. Picture yourself as explaining your arguments to somebody who is just learning about your assignment. With other words, do not assume that the person marking your assignment knows all the background and therefore you can just write down the formulas without any explanation. It is your task to convince the reader that you know what you are doing when you derive an argument. Typeset all formulas in $\\LaTeX$.|\n",
    "| Code quality | Python code should be well structured, use meaningful identifiers for variables and subroutines, and provide sufficient comments. Please refer to the examples given in the tutorials. |\n",
    "| Code efficiency | An efficient implementation of an algorithm uses fast subroutines provided by the language or additional libraries. For the purpose of implementing Machine Learning algorithms in this course, that means using the appropriate data structures provided by Python and in numpy/scipy (e.g. Linear Algebra and random generators). |\n",
    "| Late penalty | For every day (starts at midnight) after the deadline of an assignment, the mark will be reduced by 5%. No assignments shall be accepted if it is later than 10 days. | \n",
    "| Coorperation | All assignments must be done individually. Cheating and plagiarism will be dealt with in accordance with University procedures (please see the ANU policies on [Academic Honesty and Plagiarism](http://academichonesty.anu.edu.au)). Hence, for example, code for programming assignments must not be developed in groups, nor should code be shared. You are encouraged to broadly discuss ideas, approaches and techniques with a few other students, but not at a level of detail where specific solutions or implementation issues are described by anyone. If you choose to consult with other students, you will include the names of your discussion partners for each solution. If you have any questions on this, please ask the lecturer before you act. |\n",
    "| Solution | To be presented in the tutorials. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment has two parts. In the first part, you apply logistic regression to given data (maximal 13 marks). In the second part, you answer a number of questions (maximal 7 marks). All formulas and calculations which are not part of Python code should be written using $\\LaTeX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\dotprod}[2]{\\left\\langle #1, #2 \\right\\rangle}$\n",
    "$\\newcommand{\\onevec}{\\mathbb{1}}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.optimize as opt\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "\n",
    "The data set contains mass-spectrometric data which are used to distinguish between cancer and normal patterns (https://archive.ics.uci.edu/ml/datasets/Arcene). \n",
    "\n",
    "Please download the following data:\n",
    "* training data https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.data,\n",
    "* training labels https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.labels,\n",
    "* validation data https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_valid.data, and\n",
    "* validation labels https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/arcene_valid.labels.\n",
    "\n",
    "The following code reads the training and validation data into your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10000)\n",
      "(100, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.loadtxt(\"arcene_train.data\")\n",
    "y_train = np.loadtxt(\"arcene_train.labels\")\n",
    "X_val   = np.loadtxt(\"arcene_valid.data\")\n",
    "y_val   = np.loadtxt(\"arcene_valid.labels\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the 10000-dimensional input space might lead to long computation times, we have prepared a subset of 220 features. Unless otherwise stated, you will from now on work only with these subset of the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 220)\n",
      "(100, 220)\n"
     ]
    }
   ],
   "source": [
    "feature_mask = np.array(\n",
    "    [   4,   37,   85,  187,  233,  255,  375,  413,  435,  468,\n",
    "      470,  477,  519,  528,  628,  629,  643,  661,  678,  682,\n",
    "      750,  783,  786,  802,  936,  997, 1035, 1043, 1113, 1186,\n",
    "     1288, 1294, 1316, 1319, 1441, 1475, 1488, 1546, 1577, 1589,\n",
    "     1666, 1671, 1739, 1761, 1830, 1848, 1881, 1882, 1975, 2057,\n",
    "     2116, 2157, 2170, 2297, 2308, 2406, 2407, 2460, 2532, 2619,\n",
    "     2632, 2634, 2644, 2656, 2697, 2717, 2771, 2817, 2865, 2937,\n",
    "     3006, 3033, 3109, 3250, 3256, 3364, 3369, 3386, 3517, 3574,\n",
    "     3611, 3643, 3660, 3702, 3777, 3783, 3856, 4008, 4016, 4036,\n",
    "     4058, 4081, 4147, 4157, 4182, 4197, 4202, 4230, 4251, 4305,\n",
    "     4379, 4440, 4454, 4467, 4485, 4555, 4557, 4579, 4585, 4607,\n",
    "     4685, 4702, 4721, 4730, 4894, 4899, 4954, 4959, 5004, 5048,\n",
    "     5076, 5200, 5230, 5242, 5249, 5306, 5355, 5472, 5476, 5631,\n",
    "     5720, 5773, 5790, 5936, 5994, 6106, 6111, 6162, 6163, 6192,\n",
    "     6304, 6350, 6402, 6407, 6439, 6462, 6480, 6494, 6522, 6555,\n",
    "     6596, 6620, 6678, 6773, 6791, 6869, 6888, 6889, 6904, 6927,\n",
    "     6957, 6961, 7101, 7196, 7214, 7271, 7279, 7297, 7425, 7431,\n",
    "     7436, 7462, 7505, 7512, 7627, 7651, 7747, 7793, 7812, 7855,\n",
    "     7856, 7860, 7866, 7932, 7976, 7993, 8006, 8131, 8155, 8257,\n",
    "     8266, 8270, 8367, 8378, 8440, 8472, 8501, 8726, 8761, 8829,\n",
    "     8831, 8903, 9021, 9024, 9026, 9060, 9081, 9116, 9211, 9214,\n",
    "     9233, 9319, 9371, 9506, 9539, 9549, 9603, 9616, 9633, 9703])\n",
    "X_train_sub = X_train[:, feature_mask]\n",
    "X_val_sub   = X_val[:, feature_mask]\n",
    "\n",
    "# Converting -1 class to 0\n",
    "y_train[y_train==-1] = 0\n",
    "y_val[y_val==-1] = 0\n",
    "\n",
    "print(X_train_sub.shape)\n",
    "print(X_val_sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Normalise the input data\n",
    "Find a linear transformation of the training data resulting in a zero mean and unit variance. Report the parameters of the linear transformation for the first ten dimensions of the input data.\n",
    "\n",
    "In general: \n",
    "* Under which circumstances does working with this transformed data lead to an advantage? \n",
    "* When is it counterproductive to normalise input data to zero mean and/or to unit variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean for first 10 dimension: [17.109999999999999, 2.4199999999999999, 19.399999999999999, 2.8199999999999998, 35.850000000000001, 24.829999999999998, 70.010000000000005, 84.939999999999998, 20.059999999999999, 16.98]\n",
      "\n",
      "Standard Deviation for first 10 dimension: [26.215985581320417, 8.2694377076074517, 24.963172875257666, 9.0811673258452856, 42.418716387934232, 28.888078856164874, 75.096803527180839, 86.423934184923567, 25.67637824927807, 26.601872114571183]\n"
     ]
    }
   ],
   "source": [
    "def get_mean_sd(X):\n",
    "    \"\"\"Returns the mean and standard deviation of each column of the given data-set\"\"\"\n",
    "    X_mean = [ X[:,col].mean() for col in np.arange(X.shape[1])]\n",
    "    X_sd = [ X[:,col].std() for col in np.arange(X.shape[1])]\n",
    "    return (X_mean, X_sd)\n",
    "\n",
    "def normalize(X, mu, sd):\n",
    "    \"\"\" Normalizes each column of the given data-set such that \n",
    "        they have zero mean and unit-variance\n",
    "    \"\"\"\n",
    "    return (X - mu)/sd\n",
    "\n",
    "# Get the column-wise mean, sd of the training data.\n",
    "X_train_sub_mean, X_train_sub_sd = get_mean_sd(X_train_sub)\n",
    "\n",
    "# Reporting the parameters (mean, sd) for the first 10 dimensiond of input data\n",
    "print(\"Mean for first 10 dimension:\", X_train_sub_mean[:10])\n",
    "print(\"\\nStandard Deviation for first 10 dimension:\", X_train_sub_sd[:10])\n",
    "\n",
    "# Normalizing the training data and validation data using the mean, sd of the training data.\n",
    "# The reason why we use the training set (mean, sd) is to simulate the effect of the selected\n",
    "# model on an un-seen data.\n",
    "X_train_sub_norm = normalize(X_train_sub, X_train_sub_mean, X_train_sub_sd)\n",
    "X_val_sub_norm = normalize(X_val_sub, X_train_sub_mean, X_train_sub_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "##### Circumstances where normalization is an Advantage\n",
    "* When features are represented in different scale. Since the error function uses an euclidean distance to calculate error in prediction, the features with larger value (due to it's scale) get more influence on prediction when compared to features whose numerical values are small\n",
    "* When there are outliers in the data\n",
    "* When we know (assume) that the data has a normal distribution\n",
    "* When we know we are going to use optimization algorithms like gradient discent which converge much faster when the data is normalized.\n",
    "\n",
    "##### Circumstances where normalization is Counter-productive\n",
    "* When the input distribution is known to be **not** gaussian\n",
    "* When all the features are in the same range, there by rendering normalization useless\n",
    "* When some features are known to influence more on the prediction of the target, normalization is counter-productive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Error function for logistic regression with quadratic regularisation\n",
    "Define a Python function calculating the *cross-entropy* $ E(\\mathbf{w}) = - \\ln p(\\mathbf{t} \\;|\\; \\mathbf{w}) $for logistic regression with two classes and quadratic (l2) regularisation for a given parameter vector $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sig(X):\n",
    "    \"\"\"S shaped function, known as the sigmoid\"\"\"\n",
    "    return 1 / (1 + np.exp(- X))\n",
    "\n",
    "def coss_entropy_error_fn(w, X, t, l):\n",
    "    \"\"\"Return the cross-entropy value for a given set of weight vector, w\"\"\"\n",
    "    \n",
    "    # Calculating y = Sigma(a)\n",
    "    p_1 = sig(np.dot(w, X.T))\n",
    "    \n",
    "    # Calculating the two parts of the summation as dot-products.\n",
    "    erfn_1 = np.dot(t, np.log(p_1))\n",
    "    erfn_2 = np.dot((1-t), np.log(1 - p_1))\n",
    "    \n",
    "    # Evaluation of cost function value with an L2 regularization parameter.\n",
    "    cost = -1 * (erfn_1 + erfn_2) + (l/2) * np.dot(w.T, w)\n",
    "    \n",
    "    # Return a single value, cost is 1x1 array.\n",
    "    return cost.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Gradient for logistic regression with quadratic regularisation\n",
    "Define a Python function calculating the gradient of the above *cross-entropy* for logistic regression with two classes and quadratic (l2) regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(w, X, t, l):\n",
    "    \"\"\"Returns the gradient vector for a given set of weight vector, w\"\"\"\n",
    "    \n",
    "    # Calculating y = Sigma(a)\n",
    "    p_1 = sig(np.dot(w, X.T))\n",
    "    \n",
    "    # The gradient is calculated as (error in estimation)*(input) + (reg)(weights)\n",
    "    err = p_1 - t\n",
    "    grd = np.dot(err, X) + l * w    \n",
    "    return grd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Finding the optimal parameters for given regularisation\n",
    "Using the error function and the gradient defined above, you now setup the optimisation finding the optimal parameter vector $\\mathbf{w}^\\star$ for the training data and a fixed regularisation constant $\\lambda $.\n",
    "Use the function *scipy.optimize.fmin_bfgs* as optimiser.\n",
    "\n",
    "For each $\\lambda= 10^k, k=-3,-2,..,1$, report the first 10 components of the optimal parameter $\\mathbf{w}^\\star$ found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.011083\n",
      "         Iterations: 283\n",
      "         Function evaluations: 292\n",
      "         Gradient evaluations: 292\n",
      "λ: 0.001\n",
      "w⋆[:10] =  [-0.08767564 -0.19595618 -0.16850018 -0.19460629  0.54096961 -0.40664628\n",
      " -0.18763778 -0.20854873 -0.19342494 -0.06298102]\n",
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.073563\n",
      "         Iterations: 227\n",
      "         Function evaluations: 236\n",
      "         Gradient evaluations: 236\n",
      "λ: 0.01\n",
      "w⋆[:10] =  [-0.07440142 -0.15638555 -0.14045527 -0.15549846  0.4225799  -0.32751057\n",
      " -0.15189398 -0.16860158 -0.16060264 -0.05656094]\n",
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.447085\n",
      "         Iterations: 54\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 61\n",
      "λ: 0.1\n",
      "w⋆[:10] =  [-0.06142836 -0.11878849 -0.1132637  -0.11818739  0.31008382 -0.25192673\n",
      " -0.11725856 -0.13020068 -0.1286103  -0.05019121]\n",
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.386603\n",
      "         Iterations: 47\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 61\n",
      "λ: 1\n",
      "w⋆[:10] =  [-0.04916196 -0.08388047 -0.08654595 -0.08335454  0.20471708 -0.17977499\n",
      " -0.08442436 -0.09389753 -0.0972058  -0.04363477]\n",
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 10.370894\n",
      "         Iterations: 73\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 99\n",
      "λ: 10\n",
      "w⋆[:10] =  [-0.03773146 -0.05319868 -0.06049018 -0.05257699  0.11040925 -0.11177752\n",
      " -0.05449917 -0.06081203 -0.06665602 -0.03658596]\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "def add_ones(X):\n",
    "    \"\"\"Add a column of ones to the end of a given data set\"\"\"\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    M = D + 1\n",
    "    tmp = np.ones((N,M))\n",
    "    tmp[:,:-1] = X\n",
    "    return tmp\n",
    "\n",
    "def train(X, t, l, disp=1):\n",
    "    \"\"\" Returns an optimal weight vector after performing Logistic Regression \n",
    "        on the given input data set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a bias term so that the decision hyperplane has the freedom of translation.\n",
    "    X = add_ones(X)\n",
    "    M = X.shape[1]\n",
    "    \n",
    "    # Randomize sarting weights to a small value\n",
    "    w = 0.1*np.random.randn(M)\n",
    "    \n",
    "    # Find optimal weights that minimizes the cost function.\n",
    "    w_star = opt.fmin_bfgs(coss_entropy_error_fn, w, fprime=grad, args=(X, t, l,), disp=disp)\n",
    "    return w_star\n",
    "\n",
    "opt_w_map = {}\n",
    "print(\"======================================\")\n",
    "for p in range(-3,2):\n",
    "    l = 10 ** p\n",
    "    w_star = train(X_train_sub_norm, y_train, l)\n",
    "    opt_w_map[p] = w_star\n",
    "    print(\"λ:\", l)\n",
    "    print(\"w⋆[:10] = \", w_star[:10])\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Evaluating the solution with the validation data\n",
    "So far, you have only used the training data. You now apply the learned model to the validation data and compare the prediction you get with the given validation labels. \n",
    "\n",
    "Report the performance measures\n",
    "* the number of false positives (FP),\n",
    "* the number of false negatives (FN),\n",
    "* the number of true positives (TP),\n",
    "* the number of true negatives (TN),\n",
    "* the error rate,\n",
    "* the specificity, \n",
    "* the sensitivity,\n",
    "\n",
    "for the different settings of $\\lambda = 10^k, k=-3,-2,..,1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "λ: 1\n",
      "False Positives: 19\n",
      "False Negatives: 13\n",
      "True Positives: 31\n",
      "True Negatives: 37\n",
      "Error Rate: 0.32\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.7045454545454546\n",
      "======================================\n",
      "λ: 0.1\n",
      "False Positives: 19\n",
      "False Negatives: 13\n",
      "True Positives: 31\n",
      "True Negatives: 37\n",
      "Error Rate: 0.32\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.7045454545454546\n",
      "======================================\n",
      "λ: 0.001\n",
      "False Positives: 19\n",
      "False Negatives: 13\n",
      "True Positives: 31\n",
      "True Negatives: 37\n",
      "Error Rate: 0.32\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.7045454545454546\n",
      "======================================\n",
      "λ: 0.01\n",
      "False Positives: 19\n",
      "False Negatives: 12\n",
      "True Positives: 32\n",
      "True Negatives: 37\n",
      "Error Rate: 0.31\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.7272727272727273\n",
      "======================================\n",
      "λ: 10\n",
      "False Positives: 19\n",
      "False Negatives: 11\n",
      "True Positives: 33\n",
      "True Negatives: 37\n",
      "Error Rate: 0.3\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.75\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "def predict(X, w_star):\n",
    "    \"\"\"Using the learned parameter w_star, predict on validation dataset\"\"\"\n",
    "    X = add_ones(X)\n",
    "    p = sig(np.dot(w_star, X.T))\n",
    "    for i in range(len(p)):\n",
    "        if p[i] > 0.5:\n",
    "            p[i] = 1\n",
    "        else:\n",
    "            p[i] = 0\n",
    "    return p\n",
    "\n",
    "def confusion_matrix(prediction, labels):\n",
    "    \"\"\"Returns the confusion matrix for a list of predictions and (correct) labels\"\"\"\n",
    "    assert len(prediction) == len(labels)\n",
    "    def f(p, l):\n",
    "        n = 0\n",
    "        for i in range(len(prediction)):\n",
    "            if prediction[i] == p and labels[i] == l:\n",
    "                n += 1\n",
    "        return n\n",
    "    return np.matrix([[f(1, 1), f(1, 0)], [f(0, 1), f(0, 0)]])\n",
    "\n",
    "def prediction_performance(X, t, w_star, report=True):\n",
    "    \"\"\"Reports the predictive performance of the learned weights or returns the error rate\"\"\"\n",
    "    y_predicted = predict(X, w_star)\n",
    "    c_matrix = confusion_matrix(y_predicted, t)\n",
    "    tp, fp, fn, tn = c_matrix.flatten().tolist()[0]\n",
    "    \n",
    "    # Rate at which an erroneous prediction occurs\n",
    "    error_rate = (fp + fn) / (tp + fp + fn + tn)\n",
    "    \n",
    "    # AKA, True Negative Rate(TNR), is the proportion of people that tested negative and are negative\n",
    "    specificity = tn / (fp + tn)\n",
    "    \n",
    "    # AKA, True Positive Rate(TPR) or recall, is the proportion of people that tested positive and are positive\n",
    "    sensitivity = tp / (fn + tp)\n",
    "    if report:\n",
    "        print(\"False Positives:\", fp)\n",
    "        print(\"False Negatives:\", fn)\n",
    "        print(\"True Positives:\", tp)\n",
    "        print(\"True Negatives:\", tn)\n",
    "        print(\"Error Rate:\", error_rate)\n",
    "        print(\"Specificity:\", specificity)\n",
    "        print(\"Sensitivity:\", sensitivity)\n",
    "    else:\n",
    "        return error_rate\n",
    "    \n",
    "        \n",
    "print(\"======================================\")\n",
    "for p, w_star in opt_w_map.items():\n",
    "    print(\"λ:\", 10**p)\n",
    "    prediction_performance(X_val_sub_norm, y_val, w_star)\n",
    "    print(\"======================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Finding an optimal regularisation constant\n",
    "We now consider the regularisation constant as a hyperparameter which we want to optimise.\n",
    "For this task we use the training and validation data together.\n",
    "\n",
    "Implement *s-fold cross-validation* with $s = 10$ to find an optimal regularisation constant which further reduces the error rates found in the previous question. Report the \n",
    "* optimal setting for the regularisation constant,\n",
    "* the first 10 components of the optimal parameter $\\mathbf{w}^\\star$, and \n",
    "* the same performance measures as specified in the previous question which you achieved with those settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting S-fold cross-validation for λ = 0.001\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "Starting S-fold cross-validation for λ = 0.01\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "Starting S-fold cross-validation for λ = 0.1\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "Starting S-fold cross-validation for λ = 1\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "Starting S-fold cross-validation for λ = 10\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "Optimal λ: 10\n",
      "Avg. Error rate: 0.145 \n",
      "\n",
      "Training on training data with λ = 10\n",
      "Optimal w⋆[:10] =  [-0.03773156 -0.05319868 -0.06049005 -0.05257698  0.1104092  -0.11177759\n",
      " -0.05449915 -0.06081205 -0.06665589 -0.03658599]\n",
      "\n",
      "Performance measure with λ = 10\n",
      "False Positives: 19\n",
      "False Negatives: 11\n",
      "True Positives: 33\n",
      "True Negatives: 37\n",
      "Error Rate: 0.3\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.75\n"
     ]
    }
   ],
   "source": [
    "def cross_validate(S, X, t, disp=True):\n",
    "    \"\"\" Return the most Optimal value for the regularization hyper-parameter by \n",
    "        performing S-fold cross validation for all allowed values of λ.\"\"\"    \n",
    "    X_groups = np.array_split(X, S)\n",
    "    Y_groups = np.array_split(t, S)    \n",
    "    min_err_hp = (0, float(\"inf\"))\n",
    "    for p in range(-3,2):        \n",
    "        l = 10 ** p\n",
    "        if disp: print(\"Starting S-fold cross-validation for λ =\", l)\n",
    "        if disp: print(\"Training run:\", end=' ')\n",
    "        # i represents the held-out group\n",
    "        error_rates = np.ndarray(S)\n",
    "        for i in range(S):\n",
    "            if disp: print(i+1, end=' ')\n",
    "            X_others = [X_groups[x] for x in range(S) if x!=i]\n",
    "            Y_others = [Y_groups[x] for x in range(S) if x!=i]\n",
    "            X_training = np.concatenate(tuple(X_others), axis=0)\n",
    "            Y_training = np.concatenate(tuple(Y_others), axis=0)\n",
    "            w_star = train(X_training, Y_training, l, disp=0)\n",
    "\n",
    "            # Prediction on the help-out group\n",
    "            error_rates[i] = prediction_performance(X_groups[i], Y_groups[i], w_star, report=False)\n",
    "        if disp: print(\"\")\n",
    "        error_rate = error_rates.mean()\n",
    "        \n",
    "        if error_rate < min_err_hp[1]:\n",
    "            min_err_hp = (l, error_rate)\n",
    "    return min_err_hp\n",
    "\n",
    "# Use the whole data to find the optimal value for λ\n",
    "X_total = np.concatenate((X_train_sub_norm, X_val_sub_norm), axis=0)\n",
    "Y_total = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "l, avg_err_rate = cross_validate(10, X_total, Y_total)\n",
    "\n",
    "print(\"\\nOptimal λ:\", l)\n",
    "print(\"Avg. Error rate:\", avg_err_rate, \"\\n\")\n",
    "\n",
    "# Train and predict with the optimal regularization parameter.\n",
    "print(\"Training on training data with λ =\", l)\n",
    "w_ml = train(X_train_sub_norm, y_train, l, disp=0)\n",
    "print(\"Optimal w⋆[:10] = \", w_ml[:10])\n",
    "print(\"\\nPerformance measure with λ =\", l)\n",
    "prediction_performance(X_val_sub_norm, y_val, w_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Feature selection from all 10000 features\n",
    "\n",
    "In this task, you will use all 10000 features of the input data.\n",
    "\n",
    "The goal is to find a subset of the 10000 features which improves the solution found so far.\n",
    "\n",
    "An improvement is when at least one of\n",
    "* the error, or\n",
    "* the size of your chosen subset\n",
    "\n",
    "decreases.\n",
    "\n",
    "Please explain your approach and the reasons why you have chosen it.\n",
    "Provide code to find the subset and report the number of features and the error you are able to achive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "### Subset Selection Process\n",
    "\n",
    "Step 1: Use Variance Threshold filtering technique, weed out those feature having near zero variance, i.e. those feature that vary very little in the sample space.\n",
    "\n",
    "Step 2: Remove those fetures that have near zero (range: [-0.1,0.1]) correlation with the output. This means that as the value of the feature changes there is little to no change in the output. We use the Pearson product-moment correlation coefficient (`scipy.stats.pearsonr`) to get the correlation score for a feature.\n",
    "\n",
    "Step 3: Run a Greedy forward feature selection technique which ranks the features using the one-way ANOVA score (`scipy.stats.f_oneway`). Since this is a computationally intesive process, we start the checking from a feature subset size of 100, terminating the check at 300. From manual analysis we can see that subset size below 100 and above 300 give higher error rates. Run logistic regression for each subset size in the range [100,300] and chose the subset size that give the least error rate (Filter+wrapper model feature selection technique)\n",
    "\n",
    "Step 4: With the feature subset that give the least error rate, perform logistic regression and report the performance on the validation set.\n",
    "\n",
    "##### References\n",
    "* https://en.wikipedia.org/wiki/Feature_selection\n",
    "* http://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance\n",
    "* http://www.maxwellsci.com/print/rjaset/v7-625-638.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-239-b84668179f02>, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-239-b84668179f02>\"\u001b[1;36m, line \u001b[1;32m66\u001b[0m\n\u001b[1;33m    def get\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# References\n",
    "# Feature Selection: https://en.wikipedia.org/wiki/Feature_selection\n",
    "# Filtering techniques: https://en.wikipedia.org/wiki/Feature_selection#Filter_Method\n",
    "# Techiniques used: Analysis of Variance Scoring (ANOVA) and Variance Thresholding.\n",
    "\n",
    "def var_threshold(X, threshold=0):\n",
    "    \"\"\" Returns a mask of all features falling below the given variance threshold.\"\"\"\n",
    "    variances = np.var(X, axis=0)\n",
    "    if threshold==0:\n",
    "        return np.asarray([np.isclose([var],[threshold])[0] for var in variances])\n",
    "    else:\n",
    "        return np.asarray([var < threshold for var in variances])\n",
    "    \n",
    "def remove_zero_correlation(X, y, atol=10**-8):\n",
    "    \"\"\" Return a mask of all features that have their pearsonr correaltion \n",
    "        coefficient within a range around zero.\n",
    "    \"\"\"\n",
    "    feature_score = []\n",
    "    for col in range(X.shape[1]):\n",
    "        f, _ = stats.pearsonr(X[:,col], y)\n",
    "        feature_score.append(np.isclose([f], [0], atol=atol)[0])\n",
    "    return np.asarray(feature_score)\n",
    "\n",
    "def fisher_discriminant(x, y):\n",
    "    \"\"\"Return the Fisher discriminant score: ratio of between class variance to within class variance.\"\"\"\n",
    "    return ((x.mean() - y.mean())**2)/(x.var() + y.var())\n",
    "    \n",
    "def univariate_fs(X, y, k=275):\n",
    "    \"\"\" Returns a mask of the first k features ranked by the Fisher discriminant score.\"\"\"\n",
    "    feature_score = {}\n",
    "    import operator\n",
    "    for col in range(X.shape[1]):\n",
    "        f = fisher_discriminant(X[:,col], y)\n",
    "        feature_score[col] = f\n",
    "    feature_score = sorted(feature_score.items(), key=operator.itemgetter(1), reverse=True)[:k]\n",
    "    return np.asarray([col for col, _ in feature_score])\n",
    "\n",
    "\n",
    "def cross_validate_k(S, X, t, l, min_feature=100, max_feature=250,disp=True):\n",
    "    \"\"\"Performs Cross validation to optimize the number of features that gives the best error rate.\"\"\"\n",
    "    X_groups = np.array_split(X, S)\n",
    "    Y_groups = np.array_split(t, S)    \n",
    "    min_err_hp = (0, float(\"inf\"))\n",
    "    for k in range(min_feature, max_feature+1):\n",
    "        if disp: print(\"Starting S-fold cross-validation for k =\", k)\n",
    "        if disp: print(\"Training run:\", end=' ')\n",
    "        # i represents the held-out group\n",
    "        error_rates = np.ndarray(S)\n",
    "        for i in range(S):\n",
    "            if disp: print(i+1, end=' ')\n",
    "            X_others = [X_groups[x] for x in range(S) if x!=i]\n",
    "            Y_others = [Y_groups[x] for x in range(S) if x!=i]\n",
    "            X_training = np.concatenate(tuple(X_others), axis=0)\n",
    "            Y_training = np.concatenate(tuple(Y_others), axis=0)\n",
    "            \n",
    "            # Feature selection has to be done for each partition to generalize the fitting.\n",
    "            # It leads to over-fitting to the validation data if the scoring & selection is done \n",
    "            # once on the entire dataset and then masked locally in a partition.\n",
    "            mask = univariate_fs(X_training, Y_training, k=k)\n",
    "            X_training_subset = X_training[:, mask]\n",
    "            X_val_subset = X_groups[i][:, mask]\n",
    "            \n",
    "            w_star = train(X_training_subset, Y_training, l, disp=0)\n",
    "            # Prediction on the hold-out partition\n",
    "            error_rates[i] = prediction_performance(X_val_subset, Y_groups[i], w_star, report=False)        \n",
    "        error_rate = error_rates.mean()\n",
    "        if disp: print(\"; Error rate:\",error_rate)\n",
    "        if error_rate < min_err_hp[1]:\n",
    "            min_err_hp = (k, error_rate)\n",
    "    return min_err_hp\n",
    "\n",
    "def get_optimal_k(X, t, l, min_feature=100, max_feature=250, disp=False):\n",
    "    print(\"\\nStarting Greedy Forward Checking Selection Algorithm.\")\n",
    "    print(\"Performing 5-fold cross-validation with Fisher discriminant scoring.\")\n",
    "    print(\"This takes about 10 min... grab a cup of coffee :)\")\n",
    "    print(\"Minimum Feature count:\", min_feature)\n",
    "    print(\"Forward Checking Threshold:\", max_feature)\n",
    "\n",
    "    import time\n",
    "    start = time.clock()\n",
    "    k, _ = cross_validate_k(5, X, t, l, min_feature, max_feature, disp=disp)\n",
    "    stop = time.clock() - start\n",
    "    print(\"Time to find optimal k:\", stop, \"sec\")\n",
    "    return k\n",
    "\n",
    "X_total = np.concatenate((X_train, X_val), axis=0)\n",
    "Y_total = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# Apply variance threshold\n",
    "mask = var_threshold(X_total, threshold=0)\n",
    "X_total    = X_total[:, ~mask]\n",
    "\n",
    "print(\"Removed zero-variance features: \", X_total.shape[1])\n",
    "\n",
    "# Apply normalization\n",
    "X_total_mn, X_total_sd = get_mean_sd(X_total)\n",
    "X_total = normalize(X_total, X_total_mn, X_total_sd)\n",
    "\n",
    "# remove near zero correlation\n",
    "mask = remove_zero_correlation(X_total, Y_total, atol=10**-1)\n",
    "X_total = X_total[:, ~mask]\n",
    "\n",
    "print(\"Removed features with pearson correlation score in range [-0.1,0.1]: \", X_total.shape[1])\n",
    "\n",
    "X_train_tmp = np.vsplit(X, 2)[0]\n",
    "\n",
    "# Get Optmial value of k\n",
    "k = get_optimal_k(X_total, Y_total, l, min_feature=100, max_feature=220, disp=False)\n",
    "\n",
    "# The k value of 205 was found using \n",
    "mask = univariate_fs(X_total, Y_total, k=205)\n",
    "X_total = X_total[:, mask]\n",
    "\n",
    "print(\"\\nFinal selected feature subset:\", X_total.shape[1])\n",
    "\n",
    "X_train_new_sub_norm, X_val_new_sub_norm = tuple(np.vsplit(X_total, 2))\n",
    "\n",
    "w_ml = train(X_train_new_sub_norm, y_train, l)\n",
    "print(\"Optimal w⋆[:10] = \", w_ml[:10])\n",
    "print(\"\\nPerformance measure with subset:\", X_train_new_sub_norm.shape)\n",
    "prediction_performance(X_val_new_sub_norm, y_val, w_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Maximum likelihood and maximum a posteriori (MAP)\n",
    "We assume data samples $X_n = \\{ x_1,\\dots,x_n \\}$ were generated i.i.d. from a uniform distribution with unknown positive parameter $\\theta$:\n",
    "$$\n",
    "   \\mathcal{U}(x \\;|\\; 0, \\theta) = \n",
    "\\begin{cases}\n",
    " 1/\\theta & 0 \\leq x \\leq \\theta \\\\\n",
    " 0        & \\textrm{otherwise}   \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "a) We now observe four data samples $ X_4 = \\{ 6, 8, 9, 5\\}$.\n",
    "Calculate $\\theta_{ML}$, the maximum likelihood estimate of $\\theta$ for the observed data.\n",
    "\n",
    "b) Calculate the posterior distribution of $\\theta$ \n",
    "given that the data $ X_4 = \\{ 6, 8, 9, 5 \\}$ have been observed. As prior for $\\theta$\n",
    "use $p(\\theta) = \\mathcal{U}(x \\;|\\; 0, 10)$.\n",
    "\n",
    "c) Calculate $\\theta_{MAP}$, the maximum a posteriori estimate of $\\theta$ given the data $ X_4 $ and the prior $p(\\theta)$ as in the previous question.\n",
    "\n",
    "Write down the calculations in $\\LaTeX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Variance of sum of random vartiables\n",
    "Prove that the following holds for the variance of a sum of two random variables\n",
    "$ X $ and $ Y $\n",
    "$$\n",
    "\\operatorname{var}[X + Y] = \\operatorname{var}[X] + \\operatorname{var}[Y] + 2 \\operatorname{cov}[X,Y],\n",
    "$$\n",
    "where $ \\operatorname{cov}[X,Y] $ is the covariance between $X$ and $Y$.\n",
    "  \n",
    "For each step in your proof, provide a verbal explanation why this transformation step holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Matrix-vector identity proof\n",
    "Given a nonsingular matrix $ \\mathbf{A} $ and a vector $ \\mathbf{v} $ of comparable\n",
    "dimension, prove the following identity:\n",
    "$$\n",
    " (\\mathbf{A} + \\mathbf{v} \\mathbf{v}^T)^{-1} \n",
    "   = \\mathbf{A}^{-1} - \\frac{(\\mathbf{A}^{-1} \\mathbf{v}) (\\mathbf{v}^T \\mathbf{A}^{-1})}\n",
    "                       {1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Change of variance\n",
    "In Bayesian Linear Regression, the predictive distribution \n",
    "with a simplified prior \n",
    "  $ p(\\mathbf{w}  \\;|\\;  \\alpha) = \\mathcal{N}(\\mathbf{w} \\;|\\; \\mathbf{0}, \\alpha^{-1}\\mathbf{I}) $\n",
    "is a Gaussian distribution,\n",
    "$$ \n",
    "p(t  \\;|\\;  \\mathbf{x}, \\mathbf{t}, \\alpha, \\beta) \n",
    "= \\mathcal{N} (t \\;|\\; \\mathbf{m}_N^T \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x}), \\sigma_N^2(\\mathbf{x})) \n",
    "$$\n",
    "with variance\n",
    "$$\n",
    "  \\sigma_N^2(\\mathbf{x}) = \\frac{1}{\\beta} + \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x})^T \\mathbf{S}_N \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "After using another training pair $ \\left( \\mathbf{x}_{N+1}, t_{N+1} \\right) $ to adapt ($=$learn) the model,\n",
    "the variance of the predictive distribution becomes\n",
    "\n",
    "$$\n",
    "  \\sigma_{N+1}^2(\\mathbf{x}) = \\frac{1}{\\beta} + \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x})^T \\mathbf{S}_{N+1} \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "a) Define the dimensions of the variables.\n",
    "\n",
    "b) Prove that the uncertainties $ \\sigma_N^2(\\mathbf{x}) $ and\n",
    "$ \\sigma_{N+1}^2(\\mathbf{x}) $ associated with the\n",
    "predictive distributions satisfy\n",
    "\n",
    "$$\n",
    "  \\sigma_{N+1}^2(\\mathbf{x}) \\le \\sigma_N^2(\\mathbf{x}).\n",
    "$$\n",
    "*Hint: Use the Matrix-vector identity proved in the previous question.*\n",
    "\n",
    "c) Explain the meaning of this inequality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
