{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification with Logistic Regression, Bayes Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Assignment 1 (due: Monday, 18 April, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Suraj Narayanan Sasikumar\n",
    "\n",
    "Student ID: u5881495"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "|             |Notes|\n",
    "|:------------|:--|\n",
    "|Maximum marks| 20|\n",
    "|Weight|20% of final grade|\n",
    "|Format| Complete this ipython notebook. Do not forget to fill in your name and student ID above|\n",
    "|Submission mode| Use [wattle](https://wattle.anu.edu.au/)|\n",
    "|Formulas| All formulas which you derive need to be explained unless you use very common mathematical facts. Picture yourself as explaining your arguments to somebody who is just learning about your assignment. With other words, do not assume that the person marking your assignment knows all the background and therefore you can just write down the formulas without any explanation. It is your task to convince the reader that you know what you are doing when you derive an argument. Typeset all formulas in $\\LaTeX$.|\n",
    "| Code quality | Python code should be well structured, use meaningful identifiers for variables and subroutines, and provide sufficient comments. Please refer to the examples given in the tutorials. |\n",
    "| Code efficiency | An efficient implementation of an algorithm uses fast subroutines provided by the language or additional libraries. For the purpose of implementing Machine Learning algorithms in this course, that means using the appropriate data structures provided by Python and in numpy/scipy (e.g. Linear Algebra and random generators). |\n",
    "| Late penalty | For every day (starts at midnight) after the deadline of an assignment, the mark will be reduced by 5%. No assignments shall be accepted if it is later than 10 days. | \n",
    "| Coorperation | All assignments must be done individually. Cheating and plagiarism will be dealt with in accordance with University procedures (please see the ANU policies on [Academic Honesty and Plagiarism](http://academichonesty.anu.edu.au)). Hence, for example, code for programming assignments must not be developed in groups, nor should code be shared. You are encouraged to broadly discuss ideas, approaches and techniques with a few other students, but not at a level of detail where specific solutions or implementation issues are described by anyone. If you choose to consult with other students, you will include the names of your discussion partners for each solution. If you have any questions on this, please ask the lecturer before you act. |\n",
    "| Solution | To be presented in the tutorials. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment has two parts. In the first part, you apply logistic regression to given data (maximal 13 marks). In the second part, you answer a number of questions (maximal 7 marks). All formulas and calculations which are not part of Python code should be written using $\\LaTeX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\dotprod}[2]{\\left\\langle #1, #2 \\right\\rangle}$\n",
    "$\\newcommand{\\onevec}{\\mathbb{1}}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.optimize as opt\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "\n",
    "The data set contains mass-spectrometric data which are used to distinguish between cancer and normal patterns (https://archive.ics.uci.edu/ml/datasets/Arcene). \n",
    "\n",
    "Please download the following data:\n",
    "* training data https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.data,\n",
    "* training labels https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.labels,\n",
    "* validation data https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_valid.data, and\n",
    "* validation labels https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/arcene_valid.labels.\n",
    "\n",
    "The following code reads the training and validation data into your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10000)\n",
      "(100, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.loadtxt(\"arcene_train.data\")\n",
    "y_train = np.loadtxt(\"arcene_train.labels\")\n",
    "X_val   = np.loadtxt(\"arcene_valid.data\")\n",
    "y_val   = np.loadtxt(\"arcene_valid.labels\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the 10000-dimensional input space might lead to long computation times, we have prepared a subset of 220 features. Unless otherwise stated, you will from now on work only with these subset of the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 220)\n",
      "(100, 220)\n"
     ]
    }
   ],
   "source": [
    "feature_mask = np.array(\n",
    "    [   4,   37,   85,  187,  233,  255,  375,  413,  435,  468,\n",
    "      470,  477,  519,  528,  628,  629,  643,  661,  678,  682,\n",
    "      750,  783,  786,  802,  936,  997, 1035, 1043, 1113, 1186,\n",
    "     1288, 1294, 1316, 1319, 1441, 1475, 1488, 1546, 1577, 1589,\n",
    "     1666, 1671, 1739, 1761, 1830, 1848, 1881, 1882, 1975, 2057,\n",
    "     2116, 2157, 2170, 2297, 2308, 2406, 2407, 2460, 2532, 2619,\n",
    "     2632, 2634, 2644, 2656, 2697, 2717, 2771, 2817, 2865, 2937,\n",
    "     3006, 3033, 3109, 3250, 3256, 3364, 3369, 3386, 3517, 3574,\n",
    "     3611, 3643, 3660, 3702, 3777, 3783, 3856, 4008, 4016, 4036,\n",
    "     4058, 4081, 4147, 4157, 4182, 4197, 4202, 4230, 4251, 4305,\n",
    "     4379, 4440, 4454, 4467, 4485, 4555, 4557, 4579, 4585, 4607,\n",
    "     4685, 4702, 4721, 4730, 4894, 4899, 4954, 4959, 5004, 5048,\n",
    "     5076, 5200, 5230, 5242, 5249, 5306, 5355, 5472, 5476, 5631,\n",
    "     5720, 5773, 5790, 5936, 5994, 6106, 6111, 6162, 6163, 6192,\n",
    "     6304, 6350, 6402, 6407, 6439, 6462, 6480, 6494, 6522, 6555,\n",
    "     6596, 6620, 6678, 6773, 6791, 6869, 6888, 6889, 6904, 6927,\n",
    "     6957, 6961, 7101, 7196, 7214, 7271, 7279, 7297, 7425, 7431,\n",
    "     7436, 7462, 7505, 7512, 7627, 7651, 7747, 7793, 7812, 7855,\n",
    "     7856, 7860, 7866, 7932, 7976, 7993, 8006, 8131, 8155, 8257,\n",
    "     8266, 8270, 8367, 8378, 8440, 8472, 8501, 8726, 8761, 8829,\n",
    "     8831, 8903, 9021, 9024, 9026, 9060, 9081, 9116, 9211, 9214,\n",
    "     9233, 9319, 9371, 9506, 9539, 9549, 9603, 9616, 9633, 9703])\n",
    "X_train_sub = X_train[:, feature_mask]\n",
    "X_val_sub   = X_val[:, feature_mask]\n",
    "\n",
    "y_train[y_train==-1] = 0\n",
    "y_val[y_val==-1] = 0\n",
    "\n",
    "print(X_train_sub.shape)\n",
    "print(X_val_sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Normalise the input data\n",
    "Find a linear transformation of the training data resulting in a zero mean and unit variance. Report the parameters of the linear transformation for the first ten dimensions of the input data.\n",
    "\n",
    "In general: \n",
    "* Under which circumstances does working with this transformed data lead to an advantage? \n",
    "* When is it counterproductive to normalise input data to zero mean and/or to unit variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean for first 10 dimension: [17.109999999999999, 2.4199999999999999, 19.399999999999999, 2.8199999999999998, 35.850000000000001, 24.829999999999998, 70.010000000000005, 84.939999999999998, 20.059999999999999, 16.98]\n",
      "Standard Deviation for first 10 dimension: [26.215985581320417, 8.2694377076074517, 24.963172875257666, 9.0811673258452856, 42.418716387934232, 28.888078856164874, 75.096803527180839, 86.423934184923567, 25.67637824927807, 26.601872114571183]\n"
     ]
    }
   ],
   "source": [
    "def get_mean_sd(X):\n",
    "    X_mean = [ X[:,col].mean() for col in np.arange(X.shape[1])]\n",
    "    X_sd = [ X[:,col].std() for col in np.arange(X.shape[1])]\n",
    "    return (X_mean, X_sd)\n",
    "\n",
    "def normalize(X, mu, sd):\n",
    "    return (X - mu)/sd\n",
    "\n",
    "X_train_sub_mean, X_train_sub_sd = get_mean_sd(X_train_sub)\n",
    "print(\"Mean for first 10 dimension:\", X_train_sub_mean[:10])\n",
    "print(\"Standard Deviation for first 10 dimension:\", X_train_sub_sd[:10])\n",
    "X_train_sub_norm = normalize(X_train_sub, X_train_sub_mean, X_train_sub_sd)\n",
    "X_val_sub_norm = normalize(X_val_sub, X_train_sub_mean, X_train_sub_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "##### Circumstances where normalization is an Advantage\n",
    "* When features are represented in different scale. Since the error function uses an euclidean distance to calculate erro in prediction, the features with larger value (due to it's scale) get more influence on prediction when compared to features whose numerical values are small.\n",
    "* When there are outliers in the data\n",
    "* When we know that the data has a normal distribution\n",
    "* When we know we are going to use optimation algorithms like gradient discent which converge much faster when the data is normalized.\n",
    "\n",
    "##### Circumstances where normalization is Counter-productive\n",
    "* When the input distribution is known to be not gaussian\n",
    "* When all the features are in the same range, there by rendering normalization useless\n",
    "* When some features are known to influence more on the prediction of the target normalization is counter-productive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Error function for logistic regression with quadratic regularisation\n",
    "Define a Python function calculating the *cross-entropy* $ E(\\mathbf{w}) = - \\ln p(\\mathbf{t} \\;|\\; \\mathbf{w}) $for logistic regression with two classes and quadratic (l2) regularisation for a given parameter vector $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sig(X):\n",
    "    \"\"\"S shaped function, known as the sigmoid\"\"\"\n",
    "    return 1 / (1 + np.exp(- X))\n",
    "\n",
    "def coss_entropy_error_fn(w, X, t, l):\n",
    "    p_1 = sig(np.dot(w, X.T))\n",
    "    erfn_1 = np.dot(t, np.log(p_1))\n",
    "    erfn_2 = np.dot((1-t), np.log(1 - p_1))\n",
    "    cost = -1 * (erfn_1 + erfn_2) + (l/2) * np.dot(w.T, w)\n",
    "    return cost.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Gradient for logistic regression with quadratic regularisation\n",
    "Define a Python function calculating the gradient of the above *cross-entropy* for logistic regression with two classes and quadratic (l2) regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(w, X, t, l):\n",
    "    p_1 = sig(np.dot(w, X.T))\n",
    "    err = p_1 - t\n",
    "    grd = np.dot(err, X) + l * w\n",
    "    return grd\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Finding the optimal parameters for given regularisation\n",
    "Using the error function and the gradient defined above, you now setup the optimisation finding the optimal parameter vector $\\mathbf{w}^\\star$ for the training data and a fixed regularisation constant $\\lambda $.\n",
    "Use the function *scipy.optimize.fmin_bfgs* as optimiser.\n",
    "\n",
    "For each $\\lambda= 10^k, k=-3,-2,..,1$, report the first 10 components of the optimal parameter $\\mathbf{w}^\\star$ found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.011083\n",
      "         Iterations: 205\n",
      "         Function evaluations: 212\n",
      "         Gradient evaluations: 212\n",
      "λ: 0.001\n",
      "w⋆[:10] =  [-0.0879069  -0.19497055 -0.16759765 -0.19395222  0.53836321 -0.40672397\n",
      " -0.18740059 -0.20716213 -0.19243125 -0.0632721 ]\n",
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.073563\n",
      "         Iterations: 113\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 121\n",
      "λ: 0.01\n",
      "w⋆[:10] =  [-0.07444702 -0.15633863 -0.14053444 -0.15550953  0.42266379 -0.3275002\n",
      " -0.15181107 -0.16852355 -0.16072731 -0.05657318]\n",
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.447085\n",
      "         Iterations: 39\n",
      "         Function evaluations: 47\n",
      "         Gradient evaluations: 47\n",
      "λ: 0.1\n",
      "w⋆[:10] =  [-0.06143618 -0.11879678 -0.11326488 -0.11819479  0.31005748 -0.25192193\n",
      " -0.11725923 -0.13019853 -0.12860606 -0.05018903]\n",
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.386603\n",
      "         Iterations: 38\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 60\n",
      "λ: 1\n",
      "w⋆[:10] =  [-0.04916213 -0.0838807  -0.08654659 -0.08335463  0.204717   -0.17977474\n",
      " -0.08442382 -0.09389688 -0.09720628 -0.0436352 ]\n",
      "======================================\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 10.370894\n",
      "         Iterations: 62\n",
      "         Function evaluations: 85\n",
      "         Gradient evaluations: 85\n",
      "λ: 10\n",
      "w⋆[:10] =  [-0.03773156 -0.05319873 -0.0604901  -0.05257703  0.11040932 -0.11177767\n",
      " -0.05449921 -0.06081203 -0.06665594 -0.03658606]\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "def add_ones(X):\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    M = D + 1\n",
    "    tmp = np.ones((N,M))\n",
    "    tmp[:,:-1] = X\n",
    "    return tmp\n",
    "\n",
    "def train(X, t, l, disp=1):\n",
    "    X = add_ones(X)\n",
    "    M = X.shape[1]\n",
    "    w = 0.1*np.random.randn(M)\n",
    "    w_star = opt.fmin_bfgs(coss_entropy_error_fn, w, fprime=grad, args=(X, t, l,), disp=disp)\n",
    "    return w_star\n",
    "\n",
    "opt_w_map = {}\n",
    "print(\"======================================\")    \n",
    "for p in range(-3,2):\n",
    "    l = 10 ** p\n",
    "    w_star = train(X_train_sub_norm, y_train, l)\n",
    "    opt_w_map[p] = w_star\n",
    "    print(\"λ:\", l)\n",
    "    print(\"w⋆[:10] = \", w_star[:10])\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Evaluating the solution with the validation data\n",
    "So far, you have only used the training data. You now apply the learned model to the validation data and compare the prediction you get with the given validation labels. \n",
    "\n",
    "Report the performance measures\n",
    "* the number of false positives (FP),\n",
    "* the number of false negatives (FN),\n",
    "* the number of true positives (TP),\n",
    "* the number of true negatives (TN),\n",
    "* the error rate,\n",
    "* the specificity, \n",
    "* the sensitivity,\n",
    "\n",
    "for the different settings of $\\lambda = 10^k, k=-3,-2,..,1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "λ: 1\n",
      "False Positives: 19\n",
      "False Negatives: 13\n",
      "True Positives: 31\n",
      "True Negatives: 37\n",
      "Error Rate: 0.32\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.7045454545454546\n",
      "======================================\n",
      "λ: 0.1\n",
      "False Positives: 19\n",
      "False Negatives: 13\n",
      "True Positives: 31\n",
      "True Negatives: 37\n",
      "Error Rate: 0.32\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.7045454545454546\n",
      "======================================\n",
      "λ: 0.001\n",
      "False Positives: 19\n",
      "False Negatives: 13\n",
      "True Positives: 31\n",
      "True Negatives: 37\n",
      "Error Rate: 0.32\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.7045454545454546\n",
      "======================================\n",
      "λ: 0.01\n",
      "False Positives: 19\n",
      "False Negatives: 12\n",
      "True Positives: 32\n",
      "True Negatives: 37\n",
      "Error Rate: 0.31\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.7272727272727273\n",
      "======================================\n",
      "λ: 10\n",
      "False Positives: 19\n",
      "False Negatives: 11\n",
      "True Positives: 33\n",
      "True Negatives: 37\n",
      "Error Rate: 0.3\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.75\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "def predict(X, w_star):\n",
    "    \"\"\"Using the learned parameter theta_best, predict on data Xtest\"\"\"\n",
    "    X = add_ones(X)\n",
    "    p = sig(np.dot(w_star, X.T))\n",
    "    for i in range(len(p)):\n",
    "        if p[i] > 0.5:\n",
    "            p[i] = 1\n",
    "        else:\n",
    "            p[i] = 0\n",
    "    return p\n",
    "\n",
    "def confusion_matrix(prediction, labels):\n",
    "    \"\"\"Returns the confusion matrix for a list of predictions and (correct) labels\"\"\"\n",
    "    assert len(prediction) == len(labels)\n",
    "    def f(p, l):\n",
    "        n = 0\n",
    "        for i in range(len(prediction)):\n",
    "            if prediction[i] == p and labels[i] == l:\n",
    "                n += 1\n",
    "        return n\n",
    "    return np.matrix([[f(1, 1), f(1, 0)], [f(0, 1), f(0, 0)]])\n",
    "\n",
    "def prediction_performance(X, t, w_star, report=True):\n",
    "    y_predicted = predict(X, w_star)\n",
    "    c_matrix = confusion_matrix(y_predicted, t)\n",
    "    tp, fp, fn, tn = c_matrix.flatten().tolist()[0]\n",
    "    \n",
    "    # Rate at which an erroneous prediction occurs\n",
    "    error_rate = (fp + fn) / (tp + fp + fn + tn)\n",
    "    \n",
    "    # AKA, True Negative Rate(TNR), is the proportion of people that tested negative and are negative\n",
    "    specificity = tn / (fp + tn)\n",
    "    \n",
    "    # AKA, True Positive Rate(TPR) or recall, is the proportion of people that tested positive and are positive\n",
    "    sensitivity = tp / (fn + tp)\n",
    "    if report:\n",
    "        print(\"False Positives:\", fp)\n",
    "        print(\"False Negatives:\", fn)\n",
    "        print(\"True Positives:\", tp)\n",
    "        print(\"True Negatives:\", tn)\n",
    "        print(\"Error Rate:\", error_rate)\n",
    "        print(\"Specificity:\", specificity)\n",
    "        print(\"Sensitivity:\", sensitivity)\n",
    "    else:\n",
    "        return error_rate\n",
    "    \n",
    "        \n",
    "print(\"======================================\")\n",
    "for p, w_star in opt_w_map.items():\n",
    "    print(\"λ:\", 10**p)\n",
    "    prediction_performance(X_val_sub_norm, y_val, w_star)\n",
    "    print(\"======================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Finding an optimal regularisation constant\n",
    "We now consider the regularisation constant as a hyperparameter which we want to optimise.\n",
    "For this task we use the training and validation data together.\n",
    "\n",
    "Implement *s-fold cross-validation* with $s = 10$ to find an optimal regularisation constant which further reduces the error rates found in the previous question. Report the \n",
    "* optimal setting for the regularisation constant,\n",
    "* the first 10 components of the optimal parameter $\\mathbf{w}^\\star$, and \n",
    "* the same performance measures as specified in the previous question which you achieved with those settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting S-fold cross-validation for λ = 0.001\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "Starting S-fold cross-validation for λ = 0.01\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "Starting S-fold cross-validation for λ = 0.1\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "Starting S-fold cross-validation for λ = 1\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "Starting S-fold cross-validation for λ = 10\n",
      "Training run: 1 2 3 4 5 6 7 8 9 10 \n",
      "\n",
      "Optimal λ: 10\n",
      "Avg. Error rate: 0.145 \n",
      "\n",
      "Training on training data with λ = 10\n",
      "Optimal w⋆[:10] =  [-0.03773167 -0.05319874 -0.06049013 -0.05257702  0.11040925 -0.11177746\n",
      " -0.05449918 -0.06081202 -0.06665596 -0.03658613]\n",
      "\n",
      "Performance measure with λ = 10\n",
      "False Positives: 19\n",
      "False Negatives: 11\n",
      "True Positives: 33\n",
      "True Negatives: 37\n",
      "Error Rate: 0.3\n",
      "Specificity: 0.6607142857142857\n",
      "Sensitivity: 0.75\n"
     ]
    }
   ],
   "source": [
    "def s_fold_cross_validation(S, X, t, l):\n",
    "    print(\"Starting S-fold cross-validation for λ =\", l)\n",
    "    X_groups = np.array_split(X, S)\n",
    "    Y_groups = np.array_split(t, S)\n",
    "    error_rates = np.ndarray(S)\n",
    "    print(\"Training run:\", end=' ')\n",
    "    for i in range(S):\n",
    "        print(i+1, end=' ')\n",
    "        X_others = [X_groups[x] for x in range(S) if x!=i]\n",
    "        Y_others = [Y_groups[x] for x in range(S) if x!=i]\n",
    "        X_training = np.concatenate(tuple(X_others), axis=0)\n",
    "        Y_training = np.concatenate(tuple(Y_others), axis=0)\n",
    "        w_star = train(X_training, Y_training, l, disp=0)\n",
    "        error_rates[i] = prediction_performance(X_groups[i], Y_groups[i], w_star, report=False)\n",
    "    print(\"\")\n",
    "    return error_rates.mean()\n",
    "\n",
    "def optimal_reg_hyperparameter(X, t):\n",
    "    S = 10\n",
    "    min_err_hp = (0, float(\"inf\"))\n",
    "    for p in range(-3,2):\n",
    "        l = 10 ** p\n",
    "        error_rate = s_fold_cross_validation(S, X, t, l)\n",
    "        if error_rate < min_err_hp[1]:\n",
    "            min_err_hp = (l, error_rate)\n",
    "    return min_err_hp\n",
    "        \n",
    "X_total = np.concatenate((X_train_sub_norm, X_val_sub_norm), axis=0)\n",
    "Y_total = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "l, avg_err_rate = optimal_reg_hyperparameter(X_total, Y_total)\n",
    "print(\"\\nOptimal λ:\", l)\n",
    "print(\"Avg. Error rate:\", avg_err_rate, \"\\n\")\n",
    "\n",
    "print(\"Training on training data with λ =\", l)\n",
    "w_ml = train(X_train_sub_norm, y_train, l, disp=0)\n",
    "print(\"Optimal w⋆[:10] = \", w_ml[:10])\n",
    "print(\"\\nPerformance measure with λ =\", l)\n",
    "prediction_performance(X_val_sub_norm, y_val, w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Feature selection from all 10000 features\n",
    "\n",
    "In this task, you will use all 10000 features of the input data.\n",
    "\n",
    "The goal is to find a subset of the 10000 features which improves the solution found so far.\n",
    "\n",
    "An improvement is when at least one of\n",
    "* the error, or\n",
    "* the size of your chosen subset\n",
    "\n",
    "decreases.\n",
    "\n",
    "Please explain your approach and the reasons why you have chosen it.\n",
    "Provide code to find the subset and report the number of features and the error you are able to achive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "Step 1: Use Variance Threshold filtering technique, weed out those feature having less than unit variance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed zero-variance features:  9961\n",
      "Removed features with pearson correlation score in range [-0.1,0.1]:  5224\n",
      "\n",
      "Starting Greedy Forward Checking Selection Algorithm scoring with one-way ANOVA test.\n",
      "Minimum Feature count: 100\n",
      "Forward Checking Threshold: 300\n",
      "100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n",
      "\n",
      "Final selected feature subset: 275\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 26.793398\n",
      "         Iterations: 69\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 96\n",
      "Optimal w⋆[:10] =  [-0.07746375 -0.02497359  0.03890854 -0.04669153 -0.03267286  0.04118997\n",
      "  0.06097647  0.00324839  0.02340533 -0.1524282 ]\n",
      "\n",
      "Performance measure with subset: (100, 275)\n",
      "False Positives: 9\n",
      "False Negatives: 6\n",
      "True Positives: 38\n",
      "True Negatives: 47\n",
      "Error Rate: 0.15\n",
      "Specificity: 0.8392857142857143\n",
      "Sensitivity: 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "# References\n",
    "# Feature Selection: https://en.wikipedia.org/wiki/Feature_selection\n",
    "# Filtering techniques: https://en.wikipedia.org/wiki/Feature_selection#Filter_Method\n",
    "# Techiniques used: Analysis of Variance Scoring (ANOVA) and Variance Thresholding.\n",
    "\n",
    "def var_threshold(X, threshold=0):\n",
    "    variances = np.var(X, axis=0)\n",
    "    if threshold==0:\n",
    "        return np.asarray([np.isclose([var],[threshold])[0] for var in variances])\n",
    "    else:\n",
    "        return np.asarray([var < threshold for var in variances])\n",
    "    \n",
    "def remove_zero_correlation(X, y, atol=10**-8):\n",
    "    feature_score = []\n",
    "    for col in range(X.shape[1]):\n",
    "        f, _ = stats.pearsonr(X[:,col], y)\n",
    "        feature_score.append(np.isclose([f], [0], atol=atol)[0])\n",
    "    return np.asarray(feature_score)\n",
    "    \n",
    "def univariate_fs(X, y, k=400):\n",
    "    feature_score = {}\n",
    "    import operator\n",
    "    for col in range(X.shape[1]):\n",
    "        f, _ = stats.f_oneway(X[:,col], y)\n",
    "        feature_score[col] = f\n",
    "    feature_score = sorted(feature_score.items(), key=operator.itemgetter(1), reverse=True)[:k]\n",
    "    return np.asarray([col for col, _ in feature_score])\n",
    "\n",
    "X_total = np.concatenate((X_train, X_val), axis=0)\n",
    "Y_total = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# Apply variance threshold\n",
    "mask = var_threshold(X_total, threshold=0)\n",
    "X_total    = X_total[:,~mask]\n",
    "\n",
    "print(\"Removed zero-variance features: \", X_total.shape[1])\n",
    "\n",
    "# Apply normalization\n",
    "X_total_mn, X_total_sd = get_mean_sd(X_total)\n",
    "X_total = normalize(X_total, X_total_mn, X_total_sd)\n",
    "\n",
    "# remove near zero correlation\n",
    "mask = remove_zero_correlation(X_total, Y_total, atol=10**-1)\n",
    "X_total = X_total[:,~mask]\n",
    "\n",
    "print(\"Removed features with pearson correlation score in range [-0.1,0.1]: \", X_total.shape[1])\n",
    "\n",
    "X_copy = np.copy(X_total)\n",
    "\n",
    "min_feature = 100\n",
    "max_feature = 300\n",
    "print(\"\\nStarting Greedy Forward Checking Selection Algorithm, scoring with one-way ANOVA test.\")\n",
    "print(\"Minimum Feature count:\", min_feature)\n",
    "print(\"Forward Checking Threshold:\", max_feature)\n",
    "min_err_k = (0, float(\"inf\"))\n",
    "for i in range(min_feature,max_feature+1):\n",
    "    print(i, end=' ')\n",
    "    mask = univariate_fs(X_copy, Y_total, k=i)\n",
    "    X_total = X_copy[:, mask]\n",
    "    X = np.vsplit(X_total, 2)\n",
    "    w_ml = train(X[0], y_train, l, disp=0)\n",
    "    err_rate = prediction_performance(X[1], y_val, w_ml, report=False)\n",
    "    if err_rate < min_err_k[1]:\n",
    "        min_err_k = (i, err_rate)\n",
    "print(\"\")\n",
    "\n",
    "mask = univariate_fs(X_copy, Y_total, k=min_err_k[0])\n",
    "X_total = X_copy[:, mask]\n",
    "\n",
    "print(\"\\nFinal selected feature subset:\", X_total.shape[1])\n",
    "\n",
    "X = np.vsplit(X_total, 2)\n",
    "\n",
    "w_ml = train(X[0], y_train, l)\n",
    "print(\"Optimal w⋆[:10] = \", w_ml[:10])\n",
    "print(\"\\nPerformance measure with subset:\", X[0].shape)\n",
    "prediction_performance(X[1], y_val, w_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 points) Maximum likelihood and maximum a posteriori (MAP)\n",
    "We assume data samples $X_n = \\{ x_1,\\dots,x_n \\}$ were generated i.i.d. from a uniform distribution with unknown positive parameter $\\theta$:\n",
    "$$\n",
    "   \\mathcal{U}(x \\;|\\; 0, \\theta) = \n",
    "\\begin{cases}\n",
    " 1/\\theta & 0 \\leq x \\leq \\theta \\\\\n",
    " 0        & \\textrm{otherwise}   \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "a) We now observe four data samples $ X_4 = \\{ 6, 8, 9, 5\\}$.\n",
    "Calculate $\\theta_{ML}$, the maximum likelihood estimate of $\\theta$ for the observed data.\n",
    "\n",
    "b) Calculate the posterior distribution of $\\theta$ \n",
    "given that the data $ X_4 = \\{ 6, 8, 9, 5 \\}$ have been observed. As prior for $\\theta$\n",
    "use $p(\\theta) = \\mathcal{U}(x \\;|\\; 0, 10)$.\n",
    "\n",
    "c) Calculate $\\theta_{MAP}$, the maximum a posteriori estimate of $\\theta$ given the data $ X_4 $ and the prior $p(\\theta)$ as in the previous question.\n",
    "\n",
    "Write down the calculations in $\\LaTeX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Variance of sum of random vartiables\n",
    "Prove that the following holds for the variance of a sum of two random variables\n",
    "$ X $ and $ Y $\n",
    "$$\n",
    "\\operatorname{var}[X + Y] = \\operatorname{var}[X] + \\operatorname{var}[Y] + 2 \\operatorname{cov}[X,Y],\n",
    "$$\n",
    "where $ \\operatorname{cov}[X,Y] $ is the covariance between $X$ and $Y$.\n",
    "  \n",
    "For each step in your proof, provide a verbal explanation why this transformation step holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1 point) Matrix-vector identity proof\n",
    "Given a nonsingular matrix $ \\mathbf{A} $ and a vector $ \\mathbf{v} $ of comparable\n",
    "dimension, prove the following identity:\n",
    "$$\n",
    " (\\mathbf{A} + \\mathbf{v} \\mathbf{v}^T)^{-1} \n",
    "   = \\mathbf{A}^{-1} - \\frac{(\\mathbf{A}^{-1} \\mathbf{v}) (\\mathbf{v}^T \\mathbf{A}^{-1})}\n",
    "                       {1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 points) Change of variance\n",
    "In Bayesian Linear Regression, the predictive distribution \n",
    "with a simplified prior \n",
    "  $ p(\\mathbf{w}  \\;|\\;  \\alpha) = \\mathcal{N}(\\mathbf{w} \\;|\\; \\mathbf{0}, \\alpha^{-1}\\mathbf{I}) $\n",
    "is a Gaussian distribution,\n",
    "$$ \n",
    "p(t  \\;|\\;  \\mathbf{x}, \\mathbf{t}, \\alpha, \\beta) \n",
    "= \\mathcal{N} (t \\;|\\; \\mathbf{m}_N^T \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x}), \\sigma_N^2(\\mathbf{x})) \n",
    "$$\n",
    "with variance\n",
    "$$\n",
    "  \\sigma_N^2(\\mathbf{x}) = \\frac{1}{\\beta} + \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x})^T \\mathbf{S}_N \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "After using another training pair $ \\left( \\mathbf{x}_{N+1}, t_{N+1} \\right) $ to adapt ($=$learn) the model,\n",
    "the variance of the predictive distribution becomes\n",
    "\n",
    "$$\n",
    "  \\sigma_{N+1}^2(\\mathbf{x}) = \\frac{1}{\\beta} + \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x})^T \\mathbf{S}_{N+1} \\boldsymbol{\\mathsf{\\phi}}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "a) Define the dimensions of the variables.\n",
    "\n",
    "b) Prove that the uncertainties $ \\sigma_N^2(\\mathbf{x}) $ and\n",
    "$ \\sigma_{N+1}^2(\\mathbf{x}) $ associated with the\n",
    "predictive distributions satisfy\n",
    "\n",
    "$$\n",
    "  \\sigma_{N+1}^2(\\mathbf{x}) \\le \\sigma_N^2(\\mathbf{x}).\n",
    "$$\n",
    "*Hint: Use the Matrix-vector identity proved in the previous question.*\n",
    "\n",
    "c) Explain the meaning of this inequality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
